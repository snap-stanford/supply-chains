{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae913a41-f362-41b7-8b26-04dfe28e2cfd",
   "metadata": {},
   "source": [
    "# Check basic properties of SEM (raw) and SEM transaction (generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d18c155-1eb8-445a-99a3-681caa3467b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/22018/ipykernel_251650/4061657183.py:2: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sem = pd.read_csv('sem.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sem = pd.read_csv('sem.csv') # raw\n",
    "sem_trans = pd.read_csv('sem_transactions.csv') # generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47900eaa-85af-4f2b-831b-5c4fb998b295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: amount_sum, dtype: float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we remove the NaN row, as we did in preprocess_sem.py\n",
    "sem[sem.date.isna()]\n",
    "print(sem.shape)\n",
    "sem = sem[~sem.date.isna()]\n",
    "print(sem.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a304b-e644-4a5b-966a-e3664919707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No negative amount_sum values yay!\n",
    "sem.amount_sum[sem.amount_sum < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5147a-f536-44dd-888f-4336bb2965d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No negative total_amount values yay!\n",
    "sem_trans.total_amount[sem_trans.total_amount < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0aa5b3-06f1-40f9-8890-c9135123da87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6990926, 12)\n"
     ]
    }
   ],
   "source": [
    "sem_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a5b94dc-b684-40a2-b513-eae7e1504bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6990925, 12)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28632fc6-ef43-40ce-8a25-dc572615ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in generated sem_transactions.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940a3225-878f-45c9-86fd-023c8fee54fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3c0962-f526-4e0b-ae3f-31d9ce06e612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_stamp', 'supplier_t', 'buyer_t', 'hs6', 'bill_count',\n",
       "       'total_quantity', 'total_amount', 'total_weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_trans.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e61039-18de-44ba-b499-23bb753d8ed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574552    -3.949134e+06\n",
       "1032437   -3.058061e+06\n",
       "1498057   -1.496447e+06\n",
       "1869520   -2.551070e+03\n",
       "1889963   -6.008111e+05\n",
       "2426839   -2.919570e+06\n",
       "Name: total_amount, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b876443-48dd-4806-8cea-369c47f77f6b",
   "metadata": {},
   "source": [
    "# Check chronological batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490af7bd-3bbf-4da3-80eb-1078d21b787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# internal imports\n",
    "from tgb.utils.utils import *\n",
    "from tgb.linkproppred.evaluate import Evaluator\n",
    "from modules.emb_module import GraphAttentionEmbedding\n",
    "from modules.msg_func import TGNPLMessage\n",
    "from modules.msg_agg import MeanAggregator\n",
    "from modules.neighbor_loader import LastNeighborLoaderTGNPL\n",
    "from modules.memory_module import TGNPLMemory, StaticMemory\n",
    "from modules.early_stopping import  EarlyStopMonitor\n",
    "from tgb.linkproppred.dataset_pyg import PyGLinkPropPredDataset, PyGLinkPropPredDatasetHyper\n",
    "from examples.linkproppred.general.tgnpl import *\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2680e214-6683-4613-b386-c07e6d243609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate args\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "args = Namespace(dataset='tgbl-hypergraph_sem',\n",
    "                 lr=1e-4,\n",
    "                 bs=2000,  # use larger batch size since we're not training\n",
    "                 k_value=10,\n",
    "                 num_epoch=100,\n",
    "                 seed=1,\n",
    "                 mem_dim=100,\n",
    "                 time_dim=10,\n",
    "                 emb_dim=100,\n",
    "                 tolerance=1e-6,\n",
    "                 patience=100,\n",
    "                 num_run=1,\n",
    "                 wandb=False,\n",
    "                 bipartite=False,\n",
    "                 memory_name='static',\n",
    "                 emb_name='sum',\n",
    "                 use_inventory=False,\n",
    "                 debt_penalty=0,\n",
    "                 consum_rwd=0,\n",
    "                 gpu=0,\n",
    "                 num_train_days=-1,\n",
    "                 use_prev_sampling=False,\n",
    "                 batch_by_t=False)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3332cb62-2d6e-44d5-ba2c-ee1fdb8f4ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.9491e+06, -3.0581e+06, -1.4964e+06, -2.5511e+03, -6.0081e+05,\n",
       "        -2.9196e+06])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.msg[data.msg<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e2e11f-fa8d-47be-ae92-983c4acd3f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tgbl-hypergraph_sem url not found, download not supported yet.\n",
      "file found, skipping download\n",
      "Dataset directory is  /lfs/hyperturing1/0/zhiyinl/supply-chains/TGB/tgb/datasets/tgbl_hypergraph_sem\n",
      "loading processed file\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PyGLinkPropPredDatasetHyper(name\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdataset, root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      2\u001b[0m                                       use_prev_sampling \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39muse_prev_sampling)\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_TemporalData()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mset_up_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lfs/hyperturing1/0/zhiyinl/supply-chains/TGB/examples/linkproppred/general/tgnpl.py:535\u001b[0m, in \u001b[0;36mset_up_data\u001b[0;34m(args, data, dataset)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39mmsg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    534\u001b[0m     vals \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmsg[:, d]\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (vals \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall()  \u001b[38;5;66;03m# if we are logging, all values need to be positive\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# amount is first dimension\u001b[39;00m\n\u001b[1;32m    537\u001b[0m         nan_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39misnan(vals)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = PyGLinkPropPredDatasetHyper(name=args.dataset, root=\"datasets\", \n",
    "                                      use_prev_sampling = args.use_prev_sampling)\n",
    "data = dataset.get_TemporalData().to(device)\n",
    "train_loader, val_loader, test_loader = set_up_data(args, data, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155e66f-1538-44e4-85f4-2f356479a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b124a-ec2a-4895-9fe1-1c157ece523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check by assertion (timestamp value always increasing): seems correct! \n",
    "prev_max_t = 0\n",
    "for batch in tqdm(train_loader):\n",
    "    current_min_t, current_max_t = min(set(batch.t)), max(set(batch.t))\n",
    "    assert(prev_max_t <= current_min_t) # previous max should be <= current minimum\n",
    "    prev_max_t = current_max_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224eab6-d59e-41e8-b264-20034805b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check by eye-balling\n",
    "for batch in tqdm(train_loader):\n",
    "    print(batch.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb96b86-be6a-4e0d-b26e-9325047e9994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
