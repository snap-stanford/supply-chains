{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "from memory_module import TGNPLMemory\n",
    "from msg_func import TGNPLMessage\n",
    "from msg_agg import *\n",
    "\n",
    "from neighbor_loader import LastNeighborLoader, LastNeighborLoaderTGNPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test TGNPLMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 10\n",
    "num_prods = 2\n",
    "raw_msg_dim = 1\n",
    "state_dim = 10\n",
    "time_dim = 1\n",
    "message_module = TGNPLMessage(raw_msg_dim, state_dim+num_prods, time_dim)\n",
    "aggregator_module = MeanAggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 10\n"
     ]
    }
   ],
   "source": [
    "# test initialization\n",
    "mem = TGNPLMemory(num_nodes,\n",
    "        num_prods,\n",
    "        raw_msg_dim,\n",
    "        state_dim,\n",
    "        time_dim,\n",
    "        message_module,\n",
    "        aggregator_module,\n",
    "        state_updater_cell=\"gru\",\n",
    "        use_inventory=True,\n",
    "        debt_penalty=10,\n",
    "        consumption_reward=5,\n",
    "        debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 8, 4, 9, 1])\n",
      "msg_s tensor([], size=(0, 38), grad_fn=<ReluBackward0>)\n",
      "msg_d tensor([], size=(0, 38), grad_fn=<ReluBackward0>)\n",
      "msg_p tensor([], size=(0, 38), grad_fn=<ReluBackward0>)\n",
      "aggr tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Total consumed: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<IndexBackward0>)\n",
      "Total loss: tensor(0., grad_fn=<SumBackward0>)\n",
      "Total bought: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "memory tensor([[-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
      "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
      "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
      "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
      "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# before any interactions have been added\n",
    "np.random.seed(0)\n",
    "n_id = np.random.choice(num_nodes, size=5, replace=False)\n",
    "n_id = t.from_numpy(n_id)\n",
    "print(n_id)\n",
    "memory, last_update, loss = mem(n_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msg_s tensor([], size=(0, 38), grad_fn=<ReluBackward0>)\n",
      "msg_d tensor([], size=(0, 38), grad_fn=<ReluBackward0>)\n",
      "msg_p tensor([], size=(0, 38), grad_fn=<ReluBackward0>)\n",
      "aggr tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Total consumed: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<IndexBackward0>)\n",
      "Total loss: tensor(0., grad_fn=<SumBackward0>)\n",
      "Total bought: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "memory tensor([[-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
      "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
      "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
      "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
      "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
      "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
      "         -0.0609, -0.0813,  0.0000,  0.0000]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# try adding interactions\n",
    "# expectation: \n",
    "# 1) get_updated_memory will print 6 nodes, but memories shouldn't be updated yet\n",
    "# 2) _update_msg_store should update all three msg stores\n",
    "src = t.Tensor([0, 0, 1, 2]).long()\n",
    "dst = t.Tensor([3, 3, 3, 0]).long()\n",
    "prod = t.Tensor([8, 8, 8, 9]).long()\n",
    "time = t.Tensor(np.ones(4)).long()\n",
    "raw_msg = t.Tensor([10, 20, 5, 13]).reshape(-1, 1)\n",
    "mem.update_state(src, dst, prod, time, raw_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (tensor([0, 0]),\n",
       "  tensor([3, 3]),\n",
       "  tensor([8, 8]),\n",
       "  tensor([1, 1]),\n",
       "  tensor([[10.],\n",
       "          [20.]])),\n",
       " 1: (tensor([1]), tensor([3]), tensor([8]), tensor([1]), tensor([[5.]])),\n",
       " 2: (tensor([2]), tensor([0]), tensor([9]), tensor([1]), tensor([[13.]])),\n",
       " 3: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 4: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 5: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 6: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 7: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 8: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 9: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1)))}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.msg_s_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (tensor([2]), tensor([0]), tensor([9]), tensor([1]), tensor([[13.]])),\n",
       " 1: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 2: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 3: (tensor([0, 0, 1]),\n",
       "  tensor([3, 3, 3]),\n",
       "  tensor([8, 8, 8]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([[10.],\n",
       "          [20.],\n",
       "          [ 5.]])),\n",
       " 4: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 5: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 6: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 7: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 8: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 9: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1)))}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.msg_d_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 1: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 2: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 3: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 4: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 5: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 6: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 7: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 8: (tensor([0, 0, 1]),\n",
       "  tensor([3, 3, 3]),\n",
       "  tensor([8, 8, 8]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([[10.],\n",
       "          [20.],\n",
       "          [ 5.]])),\n",
       " 9: (tensor([2]), tensor([0]), tensor([9]), tensor([1]), tensor([[13.]]))}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.msg_p_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msg_s tensor([[0.0633, 0.0000, 0.9071, 0.6209, 1.2335, 0.6883, 0.1432, 0.6888, 0.5723,\n",
      "         0.9815, 0.0000, 1.1155, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3108,\n",
      "         1.1213, 0.0000, 1.1136, 1.1041, 0.3739, 0.6850, 0.0000, 0.5282, 0.0000,\n",
      "         0.0000, 1.1387, 0.0000, 0.1244, 0.0000, 0.8950, 0.0000, 0.0000, 1.0217,\n",
      "         1.3443, 0.0000],\n",
      "        [0.0615, 0.0000, 0.8504, 0.6579, 1.2221, 0.7446, 0.0964, 0.7914, 0.5253,\n",
      "         0.9308, 0.0000, 1.0441, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3535,\n",
      "         1.1436, 0.0000, 1.1878, 1.1299, 0.4242, 0.6711, 0.0000, 0.4883, 0.0000,\n",
      "         0.0000, 1.1388, 0.0000, 0.0706, 0.0000, 0.8672, 0.0000, 0.0000, 1.0399,\n",
      "         1.3335, 0.0000],\n",
      "        [0.0665, 0.0000, 1.0163, 0.5414, 1.2482, 0.5690, 0.2373, 0.4754, 0.6640,\n",
      "         1.0779, 0.0000, 1.2530, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2219,\n",
      "         1.0681, 0.0000, 0.9551, 1.0439, 0.2691, 0.7086, 0.0000, 0.6058, 0.0000,\n",
      "         0.0000, 1.1308, 0.0000, 0.2331, 0.0000, 0.9456, 0.0000, 0.0000, 0.9778,\n",
      "         1.3570, 0.0000],\n",
      "        [0.0624, 0.0000, 0.8811, 0.6382, 1.2285, 0.7146, 0.1216, 0.7366, 0.5507,\n",
      "         0.9582, 0.0000, 1.0827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3307,\n",
      "         1.1319, 0.0000, 1.1483, 1.1164, 0.3973, 0.6788, 0.0000, 0.5099, 0.0000,\n",
      "         0.0000, 1.1391, 0.0000, 0.0995, 0.0000, 0.8824, 0.0000, 0.0000, 1.0304,\n",
      "         1.3396, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "msg_d tensor([[0.0624, 0.0000, 0.8811, 0.6382, 1.2285, 0.7146, 0.1216, 0.7366, 0.5507,\n",
      "         0.9582, 0.0000, 1.0827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3307,\n",
      "         1.1319, 0.0000, 1.1483, 1.1164, 0.3973, 0.6788, 0.0000, 0.5099, 0.0000,\n",
      "         0.0000, 1.1391, 0.0000, 0.0995, 0.0000, 0.8824, 0.0000, 0.0000, 1.0304,\n",
      "         1.3396, 0.0000],\n",
      "        [0.0633, 0.0000, 0.9071, 0.6209, 1.2335, 0.6883, 0.1432, 0.6888, 0.5723,\n",
      "         0.9815, 0.0000, 1.1155, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3108,\n",
      "         1.1213, 0.0000, 1.1136, 1.1041, 0.3739, 0.6850, 0.0000, 0.5282, 0.0000,\n",
      "         0.0000, 1.1387, 0.0000, 0.1244, 0.0000, 0.8950, 0.0000, 0.0000, 1.0217,\n",
      "         1.3443, 0.0000],\n",
      "        [0.0615, 0.0000, 0.8504, 0.6579, 1.2221, 0.7446, 0.0964, 0.7914, 0.5253,\n",
      "         0.9308, 0.0000, 1.0441, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3535,\n",
      "         1.1436, 0.0000, 1.1878, 1.1299, 0.4242, 0.6711, 0.0000, 0.4883, 0.0000,\n",
      "         0.0000, 1.1388, 0.0000, 0.0706, 0.0000, 0.8672, 0.0000, 0.0000, 1.0399,\n",
      "         1.3335, 0.0000],\n",
      "        [0.0665, 0.0000, 1.0163, 0.5414, 1.2482, 0.5690, 0.2373, 0.4754, 0.6640,\n",
      "         1.0779, 0.0000, 1.2530, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2219,\n",
      "         1.0681, 0.0000, 0.9551, 1.0439, 0.2691, 0.7086, 0.0000, 0.6058, 0.0000,\n",
      "         0.0000, 1.1308, 0.0000, 0.2331, 0.0000, 0.9456, 0.0000, 0.0000, 0.9778,\n",
      "         1.3570, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "msg_p tensor([[0.0633, 0.0000, 0.9071, 0.6209, 1.2335, 0.6883, 0.1432, 0.6888, 0.5723,\n",
      "         0.9815, 0.0000, 1.1155, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3108,\n",
      "         1.1213, 0.0000, 1.1136, 1.1041, 0.3739, 0.6850, 0.0000, 0.5282, 0.0000,\n",
      "         0.0000, 1.1387, 0.0000, 0.1244, 0.0000, 0.8950, 0.0000, 0.0000, 1.0217,\n",
      "         1.3443, 0.0000],\n",
      "        [0.0615, 0.0000, 0.8504, 0.6579, 1.2221, 0.7446, 0.0964, 0.7914, 0.5253,\n",
      "         0.9308, 0.0000, 1.0441, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3535,\n",
      "         1.1436, 0.0000, 1.1878, 1.1299, 0.4242, 0.6711, 0.0000, 0.4883, 0.0000,\n",
      "         0.0000, 1.1388, 0.0000, 0.0706, 0.0000, 0.8672, 0.0000, 0.0000, 1.0399,\n",
      "         1.3335, 0.0000],\n",
      "        [0.0665, 0.0000, 1.0163, 0.5414, 1.2482, 0.5690, 0.2373, 0.4754, 0.6640,\n",
      "         1.0779, 0.0000, 1.2530, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2219,\n",
      "         1.0681, 0.0000, 0.9551, 1.0439, 0.2691, 0.7086, 0.0000, 0.6058, 0.0000,\n",
      "         0.0000, 1.1308, 0.0000, 0.2331, 0.0000, 0.9456, 0.0000, 0.0000, 0.9778,\n",
      "         1.3570, 0.0000],\n",
      "        [0.0624, 0.0000, 0.8811, 0.6382, 1.2285, 0.7146, 0.1216, 0.7366, 0.5507,\n",
      "         0.9582, 0.0000, 1.0827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3307,\n",
      "         1.1319, 0.0000, 1.1483, 1.1164, 0.3973, 0.6788, 0.0000, 0.5099, 0.0000,\n",
      "         0.0000, 1.1391, 0.0000, 0.0995, 0.0000, 0.8824, 0.0000, 0.0000, 1.0304,\n",
      "         1.3396, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "aggr tensor([[0.0624, 0.0000, 0.8795, 0.6390, 1.2280, 0.7159, 0.1204, 0.7389, 0.5494,\n",
      "         0.9568, 0.0000, 1.0807, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3316,\n",
      "         1.1323, 0.0000, 1.1499, 1.1168, 0.3985, 0.6783, 0.0000, 0.5088, 0.0000,\n",
      "         0.0000, 1.1388, 0.0000, 0.0982, 0.0000, 0.8815, 0.0000, 0.0000, 1.0307,\n",
      "         1.3391, 0.0000],\n",
      "        [0.0665, 0.0000, 1.0163, 0.5414, 1.2482, 0.5690, 0.2373, 0.4754, 0.6640,\n",
      "         1.0779, 0.0000, 1.2530, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2219,\n",
      "         1.0681, 0.0000, 0.9551, 1.0439, 0.2691, 0.7086, 0.0000, 0.6058, 0.0000,\n",
      "         0.0000, 1.1308, 0.0000, 0.2331, 0.0000, 0.9456, 0.0000, 0.0000, 0.9778,\n",
      "         1.3570, 0.0000],\n",
      "        [0.0624, 0.0000, 0.8811, 0.6382, 1.2285, 0.7146, 0.1216, 0.7366, 0.5507,\n",
      "         0.9582, 0.0000, 1.0827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3307,\n",
      "         1.1319, 0.0000, 1.1483, 1.1164, 0.3973, 0.6788, 0.0000, 0.5099, 0.0000,\n",
      "         0.0000, 1.1391, 0.0000, 0.0995, 0.0000, 0.8824, 0.0000, 0.0000, 1.0304,\n",
      "         1.3396, 0.0000],\n",
      "        [0.0637, 0.0000, 0.9246, 0.6067, 1.2346, 0.6673, 0.1590, 0.6519, 0.5872,\n",
      "         0.9967, 0.0000, 1.1375, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2954,\n",
      "         1.1110, 0.0000, 1.0855, 1.0926, 0.3557, 0.6882, 0.0000, 0.5408, 0.0000,\n",
      "         0.0000, 1.1361, 0.0000, 0.1427, 0.0000, 0.9026, 0.0000, 0.0000, 1.0131,\n",
      "         1.3449, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0637, 0.0000, 0.9246, 0.6067, 1.2346, 0.6673, 0.1590, 0.6519, 0.5872,\n",
      "         0.9967, 0.0000, 1.1375, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2954,\n",
      "         1.1110, 0.0000, 1.0855, 1.0926, 0.3557, 0.6882, 0.0000, 0.5408, 0.0000,\n",
      "         0.0000, 1.1361, 0.0000, 0.1427, 0.0000, 0.9026, 0.0000, 0.0000, 1.0131,\n",
      "         1.3449, 0.0000],\n",
      "        [0.0624, 0.0000, 0.8811, 0.6382, 1.2285, 0.7146, 0.1216, 0.7366, 0.5507,\n",
      "         0.9582, 0.0000, 1.0827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3307,\n",
      "         1.1319, 0.0000, 1.1483, 1.1164, 0.3973, 0.6788, 0.0000, 0.5099, 0.0000,\n",
      "         0.0000, 1.1391, 0.0000, 0.0995, 0.0000, 0.8824, 0.0000, 0.0000, 1.0304,\n",
      "         1.3396, 0.0000]], grad_fn=<DivBackward0>)\n",
      "Total consumed: tensor([[3.0477, 3.0477],\n",
      "        [0.5080, 0.5080],\n",
      "        [1.3207, 1.3207],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<IndexBackward0>)\n",
      "Total loss: tensor(48.7633, grad_fn=<SumBackward0>)\n",
      "Total bought: tensor([[ 0., 13.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [35.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.]])\n",
      "memory tensor([[-4.6727e-01,  2.6600e-01, -2.4418e-01,  6.1187e-01,  2.2161e-01,\n",
      "          6.5051e-01, -2.9362e-01, -6.7484e-01, -3.3671e-01, -3.5410e-01,\n",
      "         -3.0477e+00,  9.9523e+00],\n",
      "        [-4.4137e-01,  2.4492e-01, -1.9522e-01,  6.5558e-01,  3.2745e-01,\n",
      "          6.4462e-01, -3.1765e-01, -6.5007e-01, -2.9797e-01, -3.5303e-01,\n",
      "         -5.0795e-01, -5.0795e-01],\n",
      "        [-4.6713e-01,  2.6582e-01, -2.4373e-01,  6.1237e-01,  2.2263e-01,\n",
      "          6.5049e-01, -2.9391e-01, -6.7470e-01, -3.3642e-01, -3.5418e-01,\n",
      "         -1.3207e+00, -1.3207e+00],\n",
      "        [-4.5909e-01,  2.5964e-01, -2.2813e-01,  6.2657e-01,  2.5676e-01,\n",
      "          6.4862e-01, -3.0219e-01, -6.6676e-01, -3.2434e-01, -3.5494e-01,\n",
      "          3.5000e+01,  0.0000e+00],\n",
      "        [-1.1041e-01, -1.0080e-01, -9.8217e-02,  4.9765e-02,  1.7701e-01,\n",
      "          1.8546e-02,  2.1898e-02, -1.3426e-01, -6.0939e-02, -8.1320e-02,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.1041e-01, -1.0080e-01, -9.8217e-02,  4.9765e-02,  1.7701e-01,\n",
      "          1.8546e-02,  2.1898e-02, -1.3426e-01, -6.0939e-02, -8.1320e-02,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.1041e-01, -1.0080e-01, -9.8217e-02,  4.9765e-02,  1.7701e-01,\n",
      "          1.8546e-02,  2.1898e-02, -1.3426e-01, -6.0939e-02, -8.1320e-02,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.1041e-01, -1.0080e-01, -9.8217e-02,  4.9765e-02,  1.7701e-01,\n",
      "          1.8546e-02,  2.1898e-02, -1.3426e-01, -6.0939e-02, -8.1320e-02,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-4.5909e-01,  2.5964e-01, -2.2813e-01,  6.2657e-01,  2.5676e-01,\n",
      "          6.4862e-01, -3.0219e-01, -6.6676e-01, -3.2434e-01, -3.5494e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-4.6713e-01,  2.6582e-01, -2.4373e-01,  6.1237e-01,  2.2263e-01,\n",
      "          6.5049e-01, -2.9391e-01, -6.7470e-01, -3.3642e-01, -3.5418e-01,\n",
      "          0.0000e+00,  0.0000e+00]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# now get memory again - only nodes with interactions should've changed\n",
    "n_id = t.from_numpy(np.arange(num_nodes))\n",
    "memory, last_update, loss = mem(n_id)  # test .forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4671,  0.2658, -0.2437,  0.6124,  0.2226,  0.6505, -0.2939, -0.6747,\n",
       "         -0.3364, -0.3542, -1.3207, -1.3207],\n",
       "        [-0.4671,  0.2658, -0.2437,  0.6124,  0.2226,  0.6505, -0.2939, -0.6747,\n",
       "         -0.3364, -0.3542,  0.0000,  0.0000]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 and 9, same state, different inventory\n",
    "# 2 supplied product 9\n",
    "# product 9 has no inventory\n",
    "memory[[2,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4591,  0.2596, -0.2281,  0.6266,  0.2568,  0.6486, -0.3022, -0.6668,\n",
       "         -0.3243, -0.3549, 35.0000,  0.0000],\n",
       "        [-0.4591,  0.2596, -0.2281,  0.6266,  0.2568,  0.6486, -0.3022, -0.6668,\n",
       "         -0.3243, -0.3549,  0.0000,  0.0000]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 and 8, same state, different inventory\n",
    "# 3 received exactly 35 of product 8\n",
    "# product 8 has no inventory\n",
    "memory[[3,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
       "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
       "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
       "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
       "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
       "         -0.0609, -0.0813,  0.0000,  0.0000],\n",
       "        [-0.1104, -0.1008, -0.0982,  0.0498,  0.1770,  0.0185,  0.0219, -0.1343,\n",
       "         -0.0609, -0.0813,  0.0000,  0.0000]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be unaffected\n",
    "memory[[4,5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should only be updated for nodes in transactions\n",
    "last_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test attention weight learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 6\n",
    "num_prods = 3\n",
    "raw_msg_dim = 1\n",
    "state_dim = 2\n",
    "time_dim = 1\n",
    "message_module = TGNPLMessage(raw_msg_dim, state_dim+num_prods, time_dim)\n",
    "aggregator_module = MeanAggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_msg_dim + (3 * memory_dim) + time_dim\n",
    "message_module.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "n_id = t.arange(0, num_nodes).long()\n",
    "print(n_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 2\n",
      "att_weights torch.Size([3, 3])\n",
      "msg_s_module.lin.weight torch.Size([17, 17])\n",
      "msg_s_module.lin.bias torch.Size([17])\n",
      "msg_s_module.layer_norm.weight torch.Size([17])\n",
      "msg_s_module.layer_norm.bias torch.Size([17])\n",
      "msg_d_module.lin.weight torch.Size([17, 17])\n",
      "msg_d_module.lin.bias torch.Size([17])\n",
      "msg_d_module.layer_norm.weight torch.Size([17])\n",
      "msg_d_module.layer_norm.bias torch.Size([17])\n",
      "msg_p_module.lin.weight torch.Size([17, 17])\n",
      "msg_p_module.lin.bias torch.Size([17])\n",
      "msg_p_module.layer_norm.weight torch.Size([17])\n",
      "msg_p_module.layer_norm.bias torch.Size([17])\n",
      "time_enc.lin.weight torch.Size([1, 1])\n",
      "time_enc.lin.bias torch.Size([1])\n",
      "state_updater.weight_ih torch.Size([6, 17])\n",
      "state_updater.weight_hh torch.Size([6, 2])\n",
      "state_updater.bias_ih torch.Size([6])\n",
      "state_updater.bias_hh torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# test directly learning attention weights\n",
    "mem = TGNPLMemory(num_nodes,\n",
    "        num_prods,\n",
    "        raw_msg_dim,\n",
    "        state_dim,\n",
    "        time_dim,\n",
    "        message_module,\n",
    "        aggregator_module,\n",
    "        state_updater_cell=\"gru\",\n",
    "        use_inventory=True,\n",
    "        learn_att_direct=True,\n",
    "        debt_penalty=10,\n",
    "        consumption_reward=1,\n",
    "        debug=False)\n",
    "opt = t.optim.Adam(mem.parameters(), lr=1e-2)\n",
    "for name, param in mem.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "loss tensor(162., grad_fn=<SumBackward0>)\n",
      "att weights tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], grad_fn=<ReluBackward0>)\n",
      "iter 2\n",
      "loss tensor(167.3800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9900, 0.9900, 0.9900],\n",
      "        [0.9900, 0.9900, 0.9900],\n",
      "        [1.0000, 1.0000, 1.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 3\n",
      "loss tensor(352.0037, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9800, 0.9800, 0.9800],\n",
      "        [0.9800, 0.9800, 0.9800],\n",
      "        [1.0074, 1.0074, 0.9926]], grad_fn=<ReluBackward0>)\n",
      "iter 4\n",
      "loss tensor(360.1991, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9704, 0.9704, 0.9704],\n",
      "        [0.9704, 0.9704, 0.9704],\n",
      "        [1.0160, 1.0160, 0.9843]], grad_fn=<ReluBackward0>)\n",
      "iter 5\n",
      "loss tensor(541.2927, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9609, 0.9609, 0.9609],\n",
      "        [0.9609, 0.9609, 0.9609],\n",
      "        [1.0251, 1.0251, 0.9754]], grad_fn=<ReluBackward0>)\n",
      "iter 6\n",
      "loss tensor(549.2872, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9513, 0.9513, 0.9513],\n",
      "        [0.9513, 0.9513, 0.9513],\n",
      "        [1.0345, 1.0345, 0.9661]], grad_fn=<ReluBackward0>)\n",
      "iter 7\n",
      "loss tensor(726.7477, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9418, 0.9418, 0.9418],\n",
      "        [0.9418, 0.9418, 0.9418],\n",
      "        [1.0441, 1.0441, 0.9566]], grad_fn=<ReluBackward0>)\n",
      "iter 8\n",
      "loss tensor(734.5421, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9322, 0.9322, 0.9322],\n",
      "        [0.9322, 0.9322, 0.9322],\n",
      "        [1.0538, 1.0538, 0.9469]], grad_fn=<ReluBackward0>)\n",
      "iter 9\n",
      "loss tensor(908.3588, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9227, 0.9227, 0.9227],\n",
      "        [0.9227, 0.9227, 0.9227],\n",
      "        [1.0636, 1.0636, 0.9371]], grad_fn=<ReluBackward0>)\n",
      "iter 10\n",
      "loss tensor(915.9517, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9130, 0.9130, 0.9130],\n",
      "        [0.9130, 0.9130, 0.9130],\n",
      "        [1.0734, 1.0734, 0.9272]], grad_fn=<ReluBackward0>)\n",
      "iter 11\n",
      "loss tensor(1086.1206, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9036, 0.9036, 0.9036],\n",
      "        [0.9036, 0.9036, 0.9036],\n",
      "        [1.0833, 1.0833, 0.9172]], grad_fn=<ReluBackward0>)\n",
      "iter 12\n",
      "loss tensor(1093.5112, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8939, 0.8939, 0.8939],\n",
      "        [0.8939, 0.8939, 0.8939],\n",
      "        [1.0933, 1.0933, 0.9071]], grad_fn=<ReluBackward0>)\n",
      "iter 13\n",
      "loss tensor(1260.0305, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8844, 0.8844, 0.8844],\n",
      "        [0.8844, 0.8844, 0.8844],\n",
      "        [1.1033, 1.1033, 0.8971]], grad_fn=<ReluBackward0>)\n",
      "iter 14\n",
      "loss tensor(1267.2181, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8747, 0.8747, 0.8747],\n",
      "        [0.8747, 0.8747, 0.8747],\n",
      "        [1.1133, 1.1133, 0.8870]], grad_fn=<ReluBackward0>)\n",
      "iter 15\n",
      "loss tensor(1430.0874, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8652, 0.8652, 0.8652],\n",
      "        [0.8652, 0.8652, 0.8652],\n",
      "        [1.1233, 1.1233, 0.8768]], grad_fn=<ReluBackward0>)\n",
      "iter 16\n",
      "loss tensor(1437.0715, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8555, 0.8555, 0.8555],\n",
      "        [0.8555, 0.8555, 0.8555],\n",
      "        [1.1334, 1.1334, 0.8667]], grad_fn=<ReluBackward0>)\n",
      "iter 17\n",
      "loss tensor(1596.2911, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8461, 0.8461, 0.8461],\n",
      "        [0.8461, 0.8461, 0.8461],\n",
      "        [1.1434, 1.1434, 0.8565]], grad_fn=<ReluBackward0>)\n",
      "iter 18\n",
      "loss tensor(1603.0718, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8364, 0.8364, 0.8364],\n",
      "        [0.8364, 0.8364, 0.8364],\n",
      "        [1.1535, 1.1535, 0.8463]], grad_fn=<ReluBackward0>)\n",
      "iter 19\n",
      "loss tensor(1758.6426, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8270, 0.8270, 0.8270],\n",
      "        [0.8270, 0.8270, 0.8270],\n",
      "        [1.1636, 1.1636, 0.8361]], grad_fn=<ReluBackward0>)\n",
      "iter 20\n",
      "loss tensor(1765.2188, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8173, 0.8173, 0.8173],\n",
      "        [0.8173, 0.8173, 0.8173],\n",
      "        [1.1737, 1.1737, 0.8259]], grad_fn=<ReluBackward0>)\n",
      "iter 21\n",
      "loss tensor(1917.1427, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8078, 0.8078, 0.8078],\n",
      "        [0.8078, 0.8078, 0.8078],\n",
      "        [1.1838, 1.1838, 0.8157]], grad_fn=<ReluBackward0>)\n",
      "iter 22\n",
      "loss tensor(1923.5151, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7981, 0.7981, 0.7981],\n",
      "        [0.7981, 0.7981, 0.7981],\n",
      "        [1.1939, 1.1939, 0.8054]], grad_fn=<ReluBackward0>)\n",
      "iter 23\n",
      "loss tensor(2071.7932, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7887, 0.7887, 0.7887],\n",
      "        [0.7887, 0.7887, 0.7887],\n",
      "        [1.2040, 1.2040, 0.7952]], grad_fn=<ReluBackward0>)\n",
      "iter 24\n",
      "loss tensor(2077.9614, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7790, 0.7790, 0.7790],\n",
      "        [0.7790, 0.7790, 0.7790],\n",
      "        [1.2141, 1.2141, 0.7850]], grad_fn=<ReluBackward0>)\n",
      "iter 25\n",
      "loss tensor(2222.5955, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7696, 0.7696, 0.7696],\n",
      "        [0.7696, 0.7696, 0.7696],\n",
      "        [1.2242, 1.2242, 0.7748]], grad_fn=<ReluBackward0>)\n",
      "iter 26\n",
      "loss tensor(2228.5596, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7599, 0.7599, 0.7599],\n",
      "        [0.7599, 0.7599, 0.7599],\n",
      "        [1.2343, 1.2343, 0.7645]], grad_fn=<ReluBackward0>)\n",
      "iter 27\n",
      "loss tensor(2369.5513, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7505, 0.7505, 0.7505],\n",
      "        [0.7505, 0.7505, 0.7505],\n",
      "        [1.2445, 1.2445, 0.7543]], grad_fn=<ReluBackward0>)\n",
      "iter 28\n",
      "loss tensor(2375.3113, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7408, 0.7408, 0.7408],\n",
      "        [0.7408, 0.7408, 0.7408],\n",
      "        [1.2546, 1.2546, 0.7441]], grad_fn=<ReluBackward0>)\n",
      "iter 29\n",
      "loss tensor(2512.6621, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7314, 0.7314, 0.7314],\n",
      "        [0.7314, 0.7314, 0.7314],\n",
      "        [1.2647, 1.2647, 0.7338]], grad_fn=<ReluBackward0>)\n",
      "iter 30\n",
      "loss tensor(2518.2183, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7217, 0.7217, 0.7217],\n",
      "        [0.7217, 0.7217, 0.7217],\n",
      "        [1.2748, 1.2748, 0.7236]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# need products 3 and 4 to make 5\n",
    "# att weights should have nonzero in 3,1 and/or 3,2 - this works\n",
    "t.autograd.set_detect_anomaly(True)\n",
    "for i in range(1, 31):\n",
    "    opt.zero_grad()\n",
    "    if (i % 2) == 0:\n",
    "        # 1 sells 5 to 2\n",
    "        src = t.Tensor([1]).long()\n",
    "        dst = t.Tensor([2]).long()\n",
    "        prod = t.Tensor([5]).long()\n",
    "        time = t.Tensor([i]).long()\n",
    "        raw_msg = t.Tensor([1]).reshape(-1, 1)\n",
    "    else:\n",
    "        # 1 buys 3 and 4 from 0\n",
    "        src = t.Tensor([0, 0]).long()\n",
    "        dst = t.Tensor([1, 1]).long()\n",
    "        prod = t.Tensor([3, 4]).long()\n",
    "        time = t.Tensor([i, i]).long()\n",
    "        raw_msg = t.Tensor([2, 4]).reshape(-1, 1)\n",
    "\n",
    "    print('iter', i)\n",
    "    mem.update_state(src, dst, prod, time, raw_msg)\n",
    "    memory, last_update, loss = mem(n_id)\n",
    "    print('loss', loss)\n",
    "    att_weights = mem.get_prod_attention()\n",
    "    print('att weights', att_weights)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    mem.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 2\n",
      "prod_bilinear torch.Size([2, 2])\n",
      "msg_s_module.lin.weight torch.Size([17, 17])\n",
      "msg_s_module.lin.bias torch.Size([17])\n",
      "msg_s_module.layer_norm.weight torch.Size([17])\n",
      "msg_s_module.layer_norm.bias torch.Size([17])\n",
      "msg_d_module.lin.weight torch.Size([17, 17])\n",
      "msg_d_module.lin.bias torch.Size([17])\n",
      "msg_d_module.layer_norm.weight torch.Size([17])\n",
      "msg_d_module.layer_norm.bias torch.Size([17])\n",
      "msg_p_module.lin.weight torch.Size([17, 17])\n",
      "msg_p_module.lin.bias torch.Size([17])\n",
      "msg_p_module.layer_norm.weight torch.Size([17])\n",
      "msg_p_module.layer_norm.bias torch.Size([17])\n",
      "time_enc.lin.weight torch.Size([1, 1])\n",
      "time_enc.lin.bias torch.Size([1])\n",
      "state_updater.weight_ih torch.Size([6, 17])\n",
      "state_updater.weight_hh torch.Size([6, 2])\n",
      "state_updater.bias_ih torch.Size([6])\n",
      "state_updater.bias_hh torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# test learning attention weights with product memories\n",
    "mem = TGNPLMemory(num_nodes,\n",
    "        num_prods,\n",
    "        raw_msg_dim,\n",
    "        state_dim,\n",
    "        time_dim,\n",
    "        message_module,\n",
    "        aggregator_module,\n",
    "        state_updater_cell=\"gru\",\n",
    "        use_inventory=True,\n",
    "        learn_att_direct=False,\n",
    "        debt_penalty=10,\n",
    "        consumption_reward=1,\n",
    "        debug=False)\n",
    "opt = t.optim.Adam(mem.parameters(), lr=1e-2)\n",
    "for name, param in mem.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "loss tensor(0.4768, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0044, 0.0044, 0.0000],\n",
      "        [0.0044, 0.0044, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 2\n",
      "loss tensor(0.4148, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0031, 0.0031, 0.0015],\n",
      "        [0.0031, 0.0031, 0.0015],\n",
      "        [0.0015, 0.0015, 0.0005]], grad_fn=<ReluBackward0>)\n",
      "iter 3\n",
      "loss tensor(3.2606, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.1207, 0.0161, 0.0000],\n",
      "        [0.0161, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 4\n",
      "loss tensor(3.2100, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.1169, 0.0131, 0.0000],\n",
      "        [0.0131, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0202]], grad_fn=<ReluBackward0>)\n",
      "iter 5\n",
      "loss tensor(15.2349, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.1803, 0.1137, 0.0000],\n",
      "        [0.1137, 0.0697, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0142]], grad_fn=<ReluBackward0>)\n",
      "iter 6\n",
      "loss tensor(14.6644, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.1751, 0.1087, 0.0000],\n",
      "        [0.1088, 0.0651, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 7\n",
      "loss tensor(22.0765, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0960, 0.0653, 0.0000],\n",
      "        [0.0653, 0.0419, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 8\n",
      "loss tensor(21.3169, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0887, 0.0582, 0.0000],\n",
      "        [0.0582, 0.0351, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 9\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 10\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 11\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 12\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 13\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 14\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 15\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 16\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 17\n",
      "loss tensor(23.8751, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0174, 0.0243, 0.0000],\n",
      "        [0.0243, 0.0313, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 18\n",
      "loss tensor(22.9288, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0085, 0.0156, 0.0000],\n",
      "        [0.0156, 0.0226, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 19\n",
      "loss tensor(21.9199, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 20\n",
      "loss tensor(21.9199, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 21\n",
      "loss tensor(21.9199, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 22\n",
      "loss tensor(21.9918, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0080]], grad_fn=<ReluBackward0>)\n",
      "iter 23\n",
      "loss tensor(22.0076, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0046]], grad_fn=<ReluBackward0>)\n",
      "iter 24\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 25\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 26\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 27\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 28\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 29\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 30\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# need products 3 and 4 to make 5\n",
    "# att weights should have nonzero in 3,1 and/or 3,2 - this doesn't work\n",
    "t.autograd.set_detect_anomaly(True)\n",
    "for i in range(1, 31):\n",
    "    opt.zero_grad()\n",
    "    if (i % 2) == 0:\n",
    "        # 1 sells 5 to 2\n",
    "        src = t.Tensor([1]).long()\n",
    "        dst = t.Tensor([2]).long()\n",
    "        prod = t.Tensor([5]).long()\n",
    "        time = t.Tensor([i]).long()\n",
    "        raw_msg = t.Tensor([1]).reshape(-1, 1)\n",
    "    else:\n",
    "        # 1 buys 3 and 4 from 0\n",
    "        src = t.Tensor([0, 0]).long()\n",
    "        dst = t.Tensor([1, 1]).long()\n",
    "        prod = t.Tensor([3, 4]).long()\n",
    "        time = t.Tensor([i, i]).long()\n",
    "        raw_msg = t.Tensor([2, 4]).reshape(-1, 1)\n",
    "\n",
    "    print('iter', i)\n",
    "    mem.update_state(src, dst, prod, time, raw_msg)\n",
    "    memory, last_update, loss = mem(n_id)\n",
    "    print('loss', loss)\n",
    "    att_weights = mem.get_prod_attention()\n",
    "    print('att weights', att_weights)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    mem.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Neighbor Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_loader = LastNeighborLoaderTGNPL(9, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "torch.Size([9, 2])\n",
      "torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "# Test init\n",
    "print(neighbor_loader.neighbors.shape)\n",
    "print(neighbor_loader.e_id.shape)\n",
    "print(neighbor_loader._assoc.shape)\n",
    "self = neighbor_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8]) tensor([[              6, 159321811882214],\n",
      "        [              7, 158772056065462],\n",
      "        [              8,  88214333292640],\n",
      "        [              6,       110377216],\n",
      "        [              7,               0],\n",
      "        [              8,       112948880],\n",
      "        [              3,               0],\n",
      "        [              4,               1],\n",
      "        [              5,               2]])\n"
     ]
    }
   ],
   "source": [
    "# Test insert\n",
    "src = torch.Tensor([0, 1, 2]).to(torch.long)\n",
    "dst = torch.Tensor([3, 4, 5]).to(torch.long)\n",
    "prod = torch.Tensor([6, 7, 8]).to(torch.long)\n",
    "\n",
    "nodes = torch.cat([prod, src, prod, dst], dim=0)\n",
    "n_id = nodes.unique()\n",
    "neighbor_loader.insert(src, dst, prod)\n",
    "\n",
    "print(n_id, self.neighbors[n_id]) # This is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8]) torch.Size([9])\n",
      "tensor([[6, 7, 8, 6, 7, 8, 3, 0, 4, 1, 5, 2],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 6, 7, 7, 8, 8]]) torch.Size([2, 12])\n",
      "tensor([0, 1, 2, 3, 4, 5, 3, 0, 4, 1, 5, 2]) torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "# Test _call_\n",
    "f_id = torch.cat([src, dst]).unique()\n",
    "p_id = torch.cat([prod]).unique()\n",
    "\n",
    "n_id, edge_index, e_id = neighbor_loader(f_id, p_id)\n",
    "\n",
    "# Ground truth: 6 edges 0-6, 3-6, 1-7, 4-7, 2-8, 5-8\n",
    "print(n_id, n_id.shape)\n",
    "print(edge_index, edge_index.shape)\n",
    "print(e_id, e_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 6]) tensor([[6, 6],\n",
      "        [6, 7],\n",
      "        [1, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Test insert\n",
    "src = torch.Tensor([0]).to(torch.long)\n",
    "dst = torch.Tensor([1]).to(torch.long)\n",
    "prod = torch.Tensor([6]).to(torch.long)\n",
    "\n",
    "nodes = torch.cat([prod, src, prod, dst], dim=0)\n",
    "n_id = nodes.unique()\n",
    "neighbor_loader.insert(src, dst, prod)\n",
    "\n",
    "print(n_id, self.neighbors[n_id]) # This is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 6, 7]) torch.Size([4])\n",
      "tensor([[2, 2, 2, 3, 1, 0],\n",
      "        [0, 0, 1, 1, 2, 2]]) torch.Size([2, 6])\n",
      "tensor([6, 0, 7, 1, 7, 6]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Test _call_\n",
    "f_id = torch.cat([src, dst]).unique()\n",
    "p_id = torch.cat([prod]).unique()\n",
    "\n",
    "n_id, edge_index, e_id = neighbor_loader(f_id, p_id)\n",
    "\n",
    "# Ground truth: 6 edges 0-6, 1-6, 1-7\n",
    "print(n_id, n_id.shape)\n",
    "print(edge_index, edge_index.shape)\n",
    "print(e_id, e_id.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
