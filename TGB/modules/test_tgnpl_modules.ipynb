{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "from memory_module import TGNPLMemory, StaticMemory\n",
    "from msg_func import TGNPLMessage\n",
    "from msg_agg import *\n",
    "\n",
    "from neighbor_loader import LastNeighborLoader, LastNeighborLoaderTGNPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test TGNPLMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 10\n",
    "num_prods = 2\n",
    "raw_msg_dim = 1\n",
    "state_dim = 5\n",
    "time_dim = 1\n",
    "message_module = TGNPLMessage(raw_msg_dim, state_dim+num_prods, time_dim)\n",
    "aggregator_module = MeanAggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 5\n"
     ]
    }
   ],
   "source": [
    "# test initialization\n",
    "mem = TGNPLMemory(num_nodes,\n",
    "        num_prods,\n",
    "        raw_msg_dim,\n",
    "        state_dim,\n",
    "        time_dim,\n",
    "        message_module,\n",
    "        aggregator_module,\n",
    "        state_updater_cell=\"gru\",\n",
    "        use_inventory=True,\n",
    "        debt_penalty=10,\n",
    "        consumption_reward=5,\n",
    "        debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod_bilinear torch.Size([5, 5])\n",
      "msg_s_module.lin.weight torch.Size([23, 23])\n",
      "msg_s_module.lin.bias torch.Size([23])\n",
      "msg_s_module.layer_norm.weight torch.Size([23])\n",
      "msg_s_module.layer_norm.bias torch.Size([23])\n",
      "msg_d_module.lin.weight torch.Size([23, 23])\n",
      "msg_d_module.lin.bias torch.Size([23])\n",
      "msg_d_module.layer_norm.weight torch.Size([23])\n",
      "msg_d_module.layer_norm.bias torch.Size([23])\n",
      "msg_p_module.lin.weight torch.Size([23, 23])\n",
      "msg_p_module.lin.bias torch.Size([23])\n",
      "msg_p_module.layer_norm.weight torch.Size([23])\n",
      "msg_p_module.layer_norm.bias torch.Size([23])\n",
      "time_enc.lin.weight torch.Size([1, 1])\n",
      "time_enc.lin.bias torch.Size([1])\n",
      "state_updater.weight_ih torch.Size([15, 23])\n",
      "state_updater.weight_hh torch.Size([15, 5])\n",
      "state_updater.bias_ih torch.Size([15])\n",
      "state_updater.bias_hh torch.Size([15])\n",
      "init_memory.weight torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "for name, param in mem.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "init memory tensor([[ 1.2116,  0.3773,  1.0512, -0.9904,  0.2710],\n",
      "        [-0.5007,  0.8516,  0.4899, -0.1371, -0.6343],\n",
      "        [-0.2270, -2.2572, -0.2835, -0.5244,  0.9657],\n",
      "        [ 0.0474, -0.0585, -1.1629,  0.9649, -0.0190],\n",
      "        [-1.0272,  2.2202, -0.3713,  1.7193,  0.4966],\n",
      "        [ 0.8658, -2.5166, -2.2544,  0.2747, -0.1048],\n",
      "        [ 1.4423, -0.5555,  0.1374,  0.0451,  0.7957],\n",
      "        [-0.4425, -0.6752,  0.0149, -0.8949, -0.1105],\n",
      "        [ 0.3926, -0.4009,  1.2658,  2.0982, -0.7427],\n",
      "        [ 0.6506,  0.9090,  0.2477,  1.0854,  0.1493]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "msg_s tensor([], size=(0, 23), grad_fn=<ReluBackward0>)\n",
      "msg_d tensor([], size=(0, 23), grad_fn=<ReluBackward0>)\n",
      "msg_p tensor([], size=(0, 23), grad_fn=<ReluBackward0>)\n",
      "aggr tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Total consumed: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<IndexBackward0>)\n",
      "Total loss: tensor(0., grad_fn=<MeanBackward0>)\n",
      "Total bought: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "memory tensor([[ 1.2116,  0.3773,  1.0512, -0.9904,  0.2710,  0.0000,  0.0000],\n",
      "        [-0.5007,  0.8516,  0.4899, -0.1371, -0.6343,  0.0000,  0.0000],\n",
      "        [-0.2270, -2.2572, -0.2835, -0.5244,  0.9657,  0.0000,  0.0000],\n",
      "        [ 0.0474, -0.0585, -1.1629,  0.9649, -0.0190,  0.0000,  0.0000],\n",
      "        [-1.0272,  2.2202, -0.3713,  1.7193,  0.4966,  0.0000,  0.0000],\n",
      "        [ 0.8658, -2.5166, -2.2544,  0.2747, -0.1048,  0.0000,  0.0000],\n",
      "        [ 1.4423, -0.5555,  0.1374,  0.0451,  0.7957,  0.0000,  0.0000],\n",
      "        [-0.4425, -0.6752,  0.0149, -0.8949, -0.1105,  0.0000,  0.0000],\n",
      "        [ 0.3926, -0.4009,  1.2658,  2.0982, -0.7427,  0.0000,  0.0000],\n",
      "        [ 0.6506,  0.9090,  0.2477,  1.0854,  0.1493,  0.0000,  0.0000]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# before any interactions have been added\n",
    "# init memory should match memory\n",
    "n_id = t.arange(0, num_nodes, dtype=t.long)\n",
    "print(n_id)\n",
    "print('init memory', mem.init_memory(n_id))\n",
    "memory, last_update, loss = mem(n_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msg_s tensor([], size=(0, 23), grad_fn=<ReluBackward0>)\n",
      "msg_d tensor([], size=(0, 23), grad_fn=<ReluBackward0>)\n",
      "msg_p tensor([], size=(0, 23), grad_fn=<ReluBackward0>)\n",
      "aggr tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Total consumed: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<IndexBackward0>)\n",
      "Total loss: tensor(0., grad_fn=<MeanBackward0>)\n",
      "Total bought: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "memory tensor([[ 1.2116,  0.3773,  1.0512, -0.9904,  0.2710,  0.0000,  0.0000],\n",
      "        [-0.5007,  0.8516,  0.4899, -0.1371, -0.6343,  0.0000,  0.0000],\n",
      "        [-0.2270, -2.2572, -0.2835, -0.5244,  0.9657,  0.0000,  0.0000],\n",
      "        [ 0.0474, -0.0585, -1.1629,  0.9649, -0.0190,  0.0000,  0.0000],\n",
      "        [ 0.3926, -0.4009,  1.2658,  2.0982, -0.7427,  0.0000,  0.0000],\n",
      "        [ 0.6506,  0.9090,  0.2477,  1.0854,  0.1493,  0.0000,  0.0000]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# try adding interactions\n",
    "# 1) get_updated_memory will print 6 nodes [0, 1, 2, 3, 8, 9], but memories shouldn't be updated yet\n",
    "# 2) _update_msg_store should update all three msg stores\n",
    "src = t.Tensor([0, 0, 1, 2]).long()\n",
    "dst = t.Tensor([3, 3, 3, 0]).long()\n",
    "prod = t.Tensor([8, 8, 8, 9]).long()\n",
    "time = t.Tensor(np.ones(4)).long()\n",
    "raw_msg = t.Tensor([10, 20, 5, 13]).reshape(-1, 1)\n",
    "mem.update_state(src, dst, prod, time, raw_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (tensor([0, 0]),\n",
       "  tensor([3, 3]),\n",
       "  tensor([8, 8]),\n",
       "  tensor([1, 1]),\n",
       "  tensor([[10.],\n",
       "          [20.]])),\n",
       " 1: (tensor([1]), tensor([3]), tensor([8]), tensor([1]), tensor([[5.]])),\n",
       " 2: (tensor([2]), tensor([0]), tensor([9]), tensor([1]), tensor([[13.]])),\n",
       " 3: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 4: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 5: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 6: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 7: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 8: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 9: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1)))}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.msg_s_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (tensor([2]), tensor([0]), tensor([9]), tensor([1]), tensor([[13.]])),\n",
       " 1: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 2: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 3: (tensor([0, 0, 1]),\n",
       "  tensor([3, 3, 3]),\n",
       "  tensor([8, 8, 8]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([[10.],\n",
       "          [20.],\n",
       "          [ 5.]])),\n",
       " 4: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 5: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 6: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 7: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 8: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 9: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1)))}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.msg_d_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 1: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 2: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 3: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 4: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 5: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 6: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 7: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 8: (tensor([0, 0, 1]),\n",
       "  tensor([3, 3, 3]),\n",
       "  tensor([8, 8, 8]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([[10.],\n",
       "          [20.],\n",
       "          [ 5.]])),\n",
       " 9: (tensor([2]), tensor([0]), tensor([9]), tensor([1]), tensor([[13.]]))}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.msg_p_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msg_s tensor([[1.4807, 0.0000, 0.5545, 1.3220, 0.0000, 1.8267, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.7232, 0.0000, 0.0000, 1.0196, 1.5632, 0.0000, 1.1032,\n",
      "         0.0000, 0.0000, 0.2157, 0.0000, 0.0000],\n",
      "        [1.3461, 0.0000, 0.6229, 1.4381, 0.0000, 1.7520, 0.0000, 0.0000, 0.0000,\n",
      "         0.0430, 0.0292, 0.5518, 0.0000, 0.0000, 0.8611, 1.6331, 0.0000, 1.2646,\n",
      "         0.0000, 0.0000, 0.3054, 0.0000, 0.0000],\n",
      "        [1.4023, 0.0000, 0.1903, 0.5603, 0.0000, 2.2669, 0.0000, 0.0000, 0.0000,\n",
      "         0.1327, 0.0000, 0.5829, 0.0000, 0.4303, 0.9183, 1.5209, 0.0818, 1.2602,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.5099, 0.0000, 0.9403, 1.6272, 0.0000, 1.3254, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4027, 0.0000, 0.0000, 0.7330, 1.5194, 0.0000, 1.1682,\n",
      "         0.3515, 0.0000, 0.2631, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "msg_d tensor([[1.5099, 0.0000, 0.9403, 1.6272, 0.0000, 1.3254, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4027, 0.0000, 0.0000, 0.7330, 1.5194, 0.0000, 1.1682,\n",
      "         0.3515, 0.0000, 0.2631, 0.0000, 0.0000],\n",
      "        [1.4807, 0.0000, 0.5545, 1.3220, 0.0000, 1.8267, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.7232, 0.0000, 0.0000, 1.0196, 1.5632, 0.0000, 1.1032,\n",
      "         0.0000, 0.0000, 0.2157, 0.0000, 0.0000],\n",
      "        [1.3461, 0.0000, 0.6229, 1.4381, 0.0000, 1.7520, 0.0000, 0.0000, 0.0000,\n",
      "         0.0430, 0.0292, 0.5518, 0.0000, 0.0000, 0.8611, 1.6331, 0.0000, 1.2646,\n",
      "         0.0000, 0.0000, 0.3054, 0.0000, 0.0000],\n",
      "        [1.4023, 0.0000, 0.1903, 0.5603, 0.0000, 2.2669, 0.0000, 0.0000, 0.0000,\n",
      "         0.1327, 0.0000, 0.5829, 0.0000, 0.4303, 0.9183, 1.5209, 0.0818, 1.2602,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "msg_p tensor([[1.4807, 0.0000, 0.5545, 1.3220, 0.0000, 1.8267, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.7232, 0.0000, 0.0000, 1.0196, 1.5632, 0.0000, 1.1032,\n",
      "         0.0000, 0.0000, 0.2157, 0.0000, 0.0000],\n",
      "        [1.3461, 0.0000, 0.6229, 1.4381, 0.0000, 1.7520, 0.0000, 0.0000, 0.0000,\n",
      "         0.0430, 0.0292, 0.5518, 0.0000, 0.0000, 0.8611, 1.6331, 0.0000, 1.2646,\n",
      "         0.0000, 0.0000, 0.3054, 0.0000, 0.0000],\n",
      "        [1.4023, 0.0000, 0.1903, 0.5603, 0.0000, 2.2669, 0.0000, 0.0000, 0.0000,\n",
      "         0.1327, 0.0000, 0.5829, 0.0000, 0.4303, 0.9183, 1.5209, 0.0818, 1.2602,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.5099, 0.0000, 0.9403, 1.6272, 0.0000, 1.3254, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4027, 0.0000, 0.0000, 0.7330, 1.5194, 0.0000, 1.1682,\n",
      "         0.3515, 0.0000, 0.2631, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "aggr tensor([[1.4456, 0.0000, 0.7059, 1.4624, 0.0000, 1.6347, 0.0000, 0.0000, 0.0000,\n",
      "         0.0143, 0.0097, 0.5592, 0.0000, 0.0000, 0.8713, 1.5719, 0.0000, 1.1786,\n",
      "         0.1172, 0.0000, 0.2614, 0.0000, 0.0000],\n",
      "        [1.4023, 0.0000, 0.1903, 0.5603, 0.0000, 2.2669, 0.0000, 0.0000, 0.0000,\n",
      "         0.1327, 0.0000, 0.5829, 0.0000, 0.4303, 0.9183, 1.5209, 0.0818, 1.2602,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.5099, 0.0000, 0.9403, 1.6272, 0.0000, 1.3254, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4027, 0.0000, 0.0000, 0.7330, 1.5194, 0.0000, 1.1682,\n",
      "         0.3515, 0.0000, 0.2631, 0.0000, 0.0000],\n",
      "        [1.4097, 0.0000, 0.4559, 1.1068, 0.0000, 1.9485, 0.0000, 0.0000, 0.0000,\n",
      "         0.0586, 0.0097, 0.6193, 0.0000, 0.1434, 0.9330, 1.5724, 0.0273, 1.2093,\n",
      "         0.0000, 0.0000, 0.1737, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.4097, 0.0000, 0.4559, 1.1068, 0.0000, 1.9485, 0.0000, 0.0000, 0.0000,\n",
      "         0.0586, 0.0097, 0.6193, 0.0000, 0.1434, 0.9330, 1.5724, 0.0273, 1.2093,\n",
      "         0.0000, 0.0000, 0.1737, 0.0000, 0.0000],\n",
      "        [1.5099, 0.0000, 0.9403, 1.6272, 0.0000, 1.3254, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4027, 0.0000, 0.0000, 0.7330, 1.5194, 0.0000, 1.1682,\n",
      "         0.3515, 0.0000, 0.2631, 0.0000, 0.0000]], grad_fn=<DivBackward0>)\n",
      "Total consumed: tensor([[204.8347, 238.4755],\n",
      "        [ 34.1391,  39.7459],\n",
      "        [103.3394, 120.3112],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000]], grad_fn=<IndexBackward0>)\n",
      "Total loss: tensor(370.4229, grad_fn=<MeanBackward0>)\n",
      "Total bought: tensor([[ 0., 13.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [35.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.]])\n",
      "memory tensor([[ 9.6774e-01,  3.5577e-01,  7.3739e-01, -9.0616e-01,  8.4783e-01,\n",
      "         -2.0483e+02, -2.2548e+02],\n",
      "        [-8.9614e-02,  7.6891e-01,  3.3754e-01, -1.8818e-01,  4.9022e-01,\n",
      "         -3.4139e+01, -3.9746e+01],\n",
      "        [ 3.6012e-01, -1.3115e+00, -3.8810e-01, -2.2612e-01,  9.3875e-01,\n",
      "         -1.0334e+02, -1.2031e+02],\n",
      "        [ 2.0049e-01,  1.7311e-01, -1.1316e+00,  7.1657e-01,  7.7188e-01,\n",
      "          3.5000e+01,  0.0000e+00],\n",
      "        [-1.0272e+00,  2.2202e+00, -3.7127e-01,  1.7193e+00,  4.9665e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 8.6581e-01, -2.5166e+00, -2.2544e+00,  2.7472e-01, -1.0477e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.4423e+00, -5.5553e-01,  1.3739e-01,  4.5061e-02,  7.9573e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-4.4254e-01, -6.7520e-01,  1.4906e-02, -8.9491e-01, -1.1055e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 3.8768e-01, -2.1044e-01,  9.1579e-01,  1.9632e+00,  4.3385e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 5.9465e-01,  6.6270e-01,  1.1274e-01,  9.3827e-01,  7.8881e-01,\n",
      "          0.0000e+00,  0.0000e+00]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# now get memory again - only nodes with interactions should've changed\n",
    "memory, last_update, loss = mem(n_id)  # test .forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6012e-01, -1.3115e+00, -3.8810e-01, -2.2612e-01,  9.3875e-01,\n",
       "         -1.0334e+02, -1.2031e+02],\n",
       "        [ 5.9465e-01,  6.6270e-01,  1.1274e-01,  9.3827e-01,  7.8881e-01,\n",
       "          0.0000e+00,  0.0000e+00]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 supplied product 9\n",
    "# product 9 has no inventory\n",
    "memory[[2,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2005,  0.1731, -1.1316,  0.7166,  0.7719, 35.0000,  0.0000],\n",
       "        [ 0.3877, -0.2104,  0.9158,  1.9632,  0.4339,  0.0000,  0.0000]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 received exactly 35 of product 8\n",
    "# product 8 has no inventory\n",
    "memory[[3,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0272,  2.2202, -0.3713,  1.7193,  0.4966,  0.0000,  0.0000],\n",
       "        [ 0.8658, -2.5166, -2.2544,  0.2747, -0.1048,  0.0000,  0.0000],\n",
       "        [ 1.4423, -0.5555,  0.1374,  0.0451,  0.7957,  0.0000,  0.0000],\n",
       "        [-0.4425, -0.6752,  0.0149, -0.8949, -0.1105,  0.0000,  0.0000]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be unaffected\n",
    "memory[[4,5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  1,  1, -1, -1, -1, -1,  1,  1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should only be updated for nodes in transactions\n",
    "last_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msg_s tensor([[1.4807, 0.0000, 0.5545, 1.3220, 0.0000, 1.8267, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.7232, 0.0000, 0.0000, 1.0196, 1.5632, 0.0000, 1.1032,\n",
      "         0.0000, 0.0000, 0.2157, 0.0000, 0.0000],\n",
      "        [1.3461, 0.0000, 0.6229, 1.4381, 0.0000, 1.7520, 0.0000, 0.0000, 0.0000,\n",
      "         0.0430, 0.0292, 0.5518, 0.0000, 0.0000, 0.8611, 1.6331, 0.0000, 1.2646,\n",
      "         0.0000, 0.0000, 0.3054, 0.0000, 0.0000],\n",
      "        [1.4023, 0.0000, 0.1903, 0.5603, 0.0000, 2.2669, 0.0000, 0.0000, 0.0000,\n",
      "         0.1327, 0.0000, 0.5829, 0.0000, 0.4303, 0.9183, 1.5209, 0.0818, 1.2602,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.5099, 0.0000, 0.9403, 1.6272, 0.0000, 1.3254, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4027, 0.0000, 0.0000, 0.7330, 1.5194, 0.0000, 1.1682,\n",
      "         0.3515, 0.0000, 0.2631, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "msg_d tensor([[1.5099, 0.0000, 0.9403, 1.6272, 0.0000, 1.3254, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4027, 0.0000, 0.0000, 0.7330, 1.5194, 0.0000, 1.1682,\n",
      "         0.3515, 0.0000, 0.2631, 0.0000, 0.0000],\n",
      "        [1.4807, 0.0000, 0.5545, 1.3220, 0.0000, 1.8267, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.7232, 0.0000, 0.0000, 1.0196, 1.5632, 0.0000, 1.1032,\n",
      "         0.0000, 0.0000, 0.2157, 0.0000, 0.0000],\n",
      "        [1.3461, 0.0000, 0.6229, 1.4381, 0.0000, 1.7520, 0.0000, 0.0000, 0.0000,\n",
      "         0.0430, 0.0292, 0.5518, 0.0000, 0.0000, 0.8611, 1.6331, 0.0000, 1.2646,\n",
      "         0.0000, 0.0000, 0.3054, 0.0000, 0.0000],\n",
      "        [1.4023, 0.0000, 0.1903, 0.5603, 0.0000, 2.2669, 0.0000, 0.0000, 0.0000,\n",
      "         0.1327, 0.0000, 0.5829, 0.0000, 0.4303, 0.9183, 1.5209, 0.0818, 1.2602,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "msg_p tensor([[1.4807, 0.0000, 0.5545, 1.3220, 0.0000, 1.8267, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.7232, 0.0000, 0.0000, 1.0196, 1.5632, 0.0000, 1.1032,\n",
      "         0.0000, 0.0000, 0.2157, 0.0000, 0.0000],\n",
      "        [1.3461, 0.0000, 0.6229, 1.4381, 0.0000, 1.7520, 0.0000, 0.0000, 0.0000,\n",
      "         0.0430, 0.0292, 0.5518, 0.0000, 0.0000, 0.8611, 1.6331, 0.0000, 1.2646,\n",
      "         0.0000, 0.0000, 0.3054, 0.0000, 0.0000],\n",
      "        [1.4023, 0.0000, 0.1903, 0.5603, 0.0000, 2.2669, 0.0000, 0.0000, 0.0000,\n",
      "         0.1327, 0.0000, 0.5829, 0.0000, 0.4303, 0.9183, 1.5209, 0.0818, 1.2602,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.5099, 0.0000, 0.9403, 1.6272, 0.0000, 1.3254, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4027, 0.0000, 0.0000, 0.7330, 1.5194, 0.0000, 1.1682,\n",
      "         0.3515, 0.0000, 0.2631, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "aggr tensor([[1.4456, 0.0000, 0.7059, 1.4624, 0.0000, 1.6347, 0.0000, 0.0000, 0.0000,\n",
      "         0.0143, 0.0097, 0.5592, 0.0000, 0.0000, 0.8713, 1.5719, 0.0000, 1.1786,\n",
      "         0.1172, 0.0000, 0.2614, 0.0000, 0.0000],\n",
      "        [1.4023, 0.0000, 0.1903, 0.5603, 0.0000, 2.2669, 0.0000, 0.0000, 0.0000,\n",
      "         0.1327, 0.0000, 0.5829, 0.0000, 0.4303, 0.9183, 1.5209, 0.0818, 1.2602,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.5099, 0.0000, 0.9403, 1.6272, 0.0000, 1.3254, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4027, 0.0000, 0.0000, 0.7330, 1.5194, 0.0000, 1.1682,\n",
      "         0.3515, 0.0000, 0.2631, 0.0000, 0.0000],\n",
      "        [1.4097, 0.0000, 0.4559, 1.1068, 0.0000, 1.9485, 0.0000, 0.0000, 0.0000,\n",
      "         0.0586, 0.0097, 0.6193, 0.0000, 0.1434, 0.9330, 1.5724, 0.0273, 1.2093,\n",
      "         0.0000, 0.0000, 0.1737, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.4097, 0.0000, 0.4559, 1.1068, 0.0000, 1.9485, 0.0000, 0.0000, 0.0000,\n",
      "         0.0586, 0.0097, 0.6193, 0.0000, 0.1434, 0.9330, 1.5724, 0.0273, 1.2093,\n",
      "         0.0000, 0.0000, 0.1737, 0.0000, 0.0000],\n",
      "        [1.5099, 0.0000, 0.9403, 1.6272, 0.0000, 1.3254, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4027, 0.0000, 0.0000, 0.7330, 1.5194, 0.0000, 1.1682,\n",
      "         0.3515, 0.0000, 0.2631, 0.0000, 0.0000]], grad_fn=<DivBackward0>)\n",
      "Total consumed: tensor([[204.8347, 238.4755],\n",
      "        [ 34.1391,  39.7459],\n",
      "        [103.3394, 120.3112],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000]], grad_fn=<IndexBackward0>)\n",
      "Total loss: tensor(370.4229, grad_fn=<MeanBackward0>)\n",
      "Total bought: tensor([[ 0., 13.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [35.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.]])\n",
      "memory tensor([[ 9.6774e-01,  3.5577e-01,  7.3739e-01, -9.0616e-01,  8.4783e-01,\n",
      "         -2.0483e+02, -2.2548e+02],\n",
      "        [-8.9614e-02,  7.6891e-01,  3.3754e-01, -1.8818e-01,  4.9022e-01,\n",
      "         -3.4139e+01, -3.9746e+01],\n",
      "        [ 3.6012e-01, -1.3115e+00, -3.8810e-01, -2.2612e-01,  9.3875e-01,\n",
      "         -1.0334e+02, -1.2031e+02],\n",
      "        [ 2.0049e-01,  1.7311e-01, -1.1316e+00,  7.1657e-01,  7.7188e-01,\n",
      "          3.5000e+01,  0.0000e+00],\n",
      "        [-1.0272e+00,  2.2202e+00, -3.7127e-01,  1.7193e+00,  4.9665e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 8.6581e-01, -2.5166e+00, -2.2544e+00,  2.7472e-01, -1.0477e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.4423e+00, -5.5553e-01,  1.3739e-01,  4.5061e-02,  7.9573e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-4.4254e-01, -6.7520e-01,  1.4906e-02, -8.9491e-01, -1.1055e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 3.8768e-01, -2.1044e-01,  9.1579e-01,  1.9632e+00,  4.3385e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 5.9465e-01,  6.6270e-01,  1.1274e-01,  9.3827e-01,  7.8881e-01,\n",
      "          0.0000e+00,  0.0000e+00]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# store latest memory, reset message store\n",
    "mem._update_memory(n_id)\n",
    "mem._reset_message_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.6774e-01,  3.5577e-01,  7.3739e-01, -9.0616e-01,  8.4783e-01,\n",
       "         -2.0483e+02, -2.2548e+02],\n",
       "        [-8.9614e-02,  7.6891e-01,  3.3754e-01, -1.8818e-01,  4.9022e-01,\n",
       "         -3.4139e+01, -3.9746e+01],\n",
       "        [ 3.6012e-01, -1.3115e+00, -3.8810e-01, -2.2612e-01,  9.3875e-01,\n",
       "         -1.0334e+02, -1.2031e+02],\n",
       "        [ 2.0049e-01,  1.7311e-01, -1.1316e+00,  7.1657e-01,  7.7188e-01,\n",
       "          3.5000e+01,  0.0000e+00],\n",
       "        [-1.0272e+00,  2.2202e+00, -3.7127e-01,  1.7193e+00,  4.9665e-01,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 8.6581e-01, -2.5166e+00, -2.2544e+00,  2.7472e-01, -1.0477e-01,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.4423e+00, -5.5553e-01,  1.3739e-01,  4.5061e-02,  7.9573e-01,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-4.4254e-01, -6.7520e-01,  1.4906e-02, -8.9491e-01, -1.1055e-01,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 3.8768e-01, -2.1044e-01,  9.1579e-01,  1.9632e+00,  4.3385e-01,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 5.9465e-01,  6.6270e-01,  1.1274e-01,  9.3827e-01,  7.8881e-01,\n",
       "          0.0000e+00,  0.0000e+00]], grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be equal when node hasn't had interactions\n",
    "mem.memory[:, :5] == mem.init_memory(n_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 1: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 2: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 3: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 4: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 5: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 6: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 7: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 8: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 9: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1)))}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be empty\n",
    "mem.msg_s_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  1,  1, -1, -1, -1, -1,  1,  1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.last_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test attention weight learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 6\n",
    "num_prods = 3\n",
    "raw_msg_dim = 1\n",
    "state_dim = 2\n",
    "time_dim = 1\n",
    "message_module = TGNPLMessage(raw_msg_dim, state_dim+num_prods, time_dim)\n",
    "aggregator_module = MeanAggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_msg_dim + (3 * memory_dim) + time_dim\n",
    "message_module.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "n_id = t.arange(0, num_nodes).long()\n",
    "print(n_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 2\n",
      "att_weights torch.Size([3, 3])\n",
      "msg_s_module.lin.weight torch.Size([17, 17])\n",
      "msg_s_module.lin.bias torch.Size([17])\n",
      "msg_s_module.layer_norm.weight torch.Size([17])\n",
      "msg_s_module.layer_norm.bias torch.Size([17])\n",
      "msg_d_module.lin.weight torch.Size([17, 17])\n",
      "msg_d_module.lin.bias torch.Size([17])\n",
      "msg_d_module.layer_norm.weight torch.Size([17])\n",
      "msg_d_module.layer_norm.bias torch.Size([17])\n",
      "msg_p_module.lin.weight torch.Size([17, 17])\n",
      "msg_p_module.lin.bias torch.Size([17])\n",
      "msg_p_module.layer_norm.weight torch.Size([17])\n",
      "msg_p_module.layer_norm.bias torch.Size([17])\n",
      "time_enc.lin.weight torch.Size([1, 1])\n",
      "time_enc.lin.bias torch.Size([1])\n",
      "state_updater.weight_ih torch.Size([6, 17])\n",
      "state_updater.weight_hh torch.Size([6, 2])\n",
      "state_updater.bias_ih torch.Size([6])\n",
      "state_updater.bias_hh torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# test directly learning attention weights\n",
    "mem = TGNPLMemory(num_nodes,\n",
    "        num_prods,\n",
    "        raw_msg_dim,\n",
    "        state_dim,\n",
    "        time_dim,\n",
    "        message_module,\n",
    "        aggregator_module,\n",
    "        state_updater_cell=\"gru\",\n",
    "        use_inventory=True,\n",
    "        learn_att_direct=True,\n",
    "        debt_penalty=10,\n",
    "        consumption_reward=1,\n",
    "        debug=False)\n",
    "opt = t.optim.Adam(mem.parameters(), lr=1e-2)\n",
    "for name, param in mem.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "loss tensor(162., grad_fn=<SumBackward0>)\n",
      "att weights tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], grad_fn=<ReluBackward0>)\n",
      "iter 2\n",
      "loss tensor(167.3800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9900, 0.9900, 0.9900],\n",
      "        [0.9900, 0.9900, 0.9900],\n",
      "        [1.0000, 1.0000, 1.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 3\n",
      "loss tensor(352.0037, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9800, 0.9800, 0.9800],\n",
      "        [0.9800, 0.9800, 0.9800],\n",
      "        [1.0074, 1.0074, 0.9926]], grad_fn=<ReluBackward0>)\n",
      "iter 4\n",
      "loss tensor(360.1991, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9704, 0.9704, 0.9704],\n",
      "        [0.9704, 0.9704, 0.9704],\n",
      "        [1.0160, 1.0160, 0.9843]], grad_fn=<ReluBackward0>)\n",
      "iter 5\n",
      "loss tensor(541.2927, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9609, 0.9609, 0.9609],\n",
      "        [0.9609, 0.9609, 0.9609],\n",
      "        [1.0251, 1.0251, 0.9754]], grad_fn=<ReluBackward0>)\n",
      "iter 6\n",
      "loss tensor(549.2872, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9513, 0.9513, 0.9513],\n",
      "        [0.9513, 0.9513, 0.9513],\n",
      "        [1.0345, 1.0345, 0.9661]], grad_fn=<ReluBackward0>)\n",
      "iter 7\n",
      "loss tensor(726.7477, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9418, 0.9418, 0.9418],\n",
      "        [0.9418, 0.9418, 0.9418],\n",
      "        [1.0441, 1.0441, 0.9566]], grad_fn=<ReluBackward0>)\n",
      "iter 8\n",
      "loss tensor(734.5421, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9322, 0.9322, 0.9322],\n",
      "        [0.9322, 0.9322, 0.9322],\n",
      "        [1.0538, 1.0538, 0.9469]], grad_fn=<ReluBackward0>)\n",
      "iter 9\n",
      "loss tensor(908.3588, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9227, 0.9227, 0.9227],\n",
      "        [0.9227, 0.9227, 0.9227],\n",
      "        [1.0636, 1.0636, 0.9371]], grad_fn=<ReluBackward0>)\n",
      "iter 10\n",
      "loss tensor(915.9517, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9130, 0.9130, 0.9130],\n",
      "        [0.9130, 0.9130, 0.9130],\n",
      "        [1.0734, 1.0734, 0.9272]], grad_fn=<ReluBackward0>)\n",
      "iter 11\n",
      "loss tensor(1086.1206, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.9036, 0.9036, 0.9036],\n",
      "        [0.9036, 0.9036, 0.9036],\n",
      "        [1.0833, 1.0833, 0.9172]], grad_fn=<ReluBackward0>)\n",
      "iter 12\n",
      "loss tensor(1093.5112, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8939, 0.8939, 0.8939],\n",
      "        [0.8939, 0.8939, 0.8939],\n",
      "        [1.0933, 1.0933, 0.9071]], grad_fn=<ReluBackward0>)\n",
      "iter 13\n",
      "loss tensor(1260.0305, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8844, 0.8844, 0.8844],\n",
      "        [0.8844, 0.8844, 0.8844],\n",
      "        [1.1033, 1.1033, 0.8971]], grad_fn=<ReluBackward0>)\n",
      "iter 14\n",
      "loss tensor(1267.2181, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8747, 0.8747, 0.8747],\n",
      "        [0.8747, 0.8747, 0.8747],\n",
      "        [1.1133, 1.1133, 0.8870]], grad_fn=<ReluBackward0>)\n",
      "iter 15\n",
      "loss tensor(1430.0874, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8652, 0.8652, 0.8652],\n",
      "        [0.8652, 0.8652, 0.8652],\n",
      "        [1.1233, 1.1233, 0.8768]], grad_fn=<ReluBackward0>)\n",
      "iter 16\n",
      "loss tensor(1437.0715, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8555, 0.8555, 0.8555],\n",
      "        [0.8555, 0.8555, 0.8555],\n",
      "        [1.1334, 1.1334, 0.8667]], grad_fn=<ReluBackward0>)\n",
      "iter 17\n",
      "loss tensor(1596.2911, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8461, 0.8461, 0.8461],\n",
      "        [0.8461, 0.8461, 0.8461],\n",
      "        [1.1434, 1.1434, 0.8565]], grad_fn=<ReluBackward0>)\n",
      "iter 18\n",
      "loss tensor(1603.0718, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8364, 0.8364, 0.8364],\n",
      "        [0.8364, 0.8364, 0.8364],\n",
      "        [1.1535, 1.1535, 0.8463]], grad_fn=<ReluBackward0>)\n",
      "iter 19\n",
      "loss tensor(1758.6426, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8270, 0.8270, 0.8270],\n",
      "        [0.8270, 0.8270, 0.8270],\n",
      "        [1.1636, 1.1636, 0.8361]], grad_fn=<ReluBackward0>)\n",
      "iter 20\n",
      "loss tensor(1765.2188, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8173, 0.8173, 0.8173],\n",
      "        [0.8173, 0.8173, 0.8173],\n",
      "        [1.1737, 1.1737, 0.8259]], grad_fn=<ReluBackward0>)\n",
      "iter 21\n",
      "loss tensor(1917.1427, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.8078, 0.8078, 0.8078],\n",
      "        [0.8078, 0.8078, 0.8078],\n",
      "        [1.1838, 1.1838, 0.8157]], grad_fn=<ReluBackward0>)\n",
      "iter 22\n",
      "loss tensor(1923.5151, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7981, 0.7981, 0.7981],\n",
      "        [0.7981, 0.7981, 0.7981],\n",
      "        [1.1939, 1.1939, 0.8054]], grad_fn=<ReluBackward0>)\n",
      "iter 23\n",
      "loss tensor(2071.7932, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7887, 0.7887, 0.7887],\n",
      "        [0.7887, 0.7887, 0.7887],\n",
      "        [1.2040, 1.2040, 0.7952]], grad_fn=<ReluBackward0>)\n",
      "iter 24\n",
      "loss tensor(2077.9614, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7790, 0.7790, 0.7790],\n",
      "        [0.7790, 0.7790, 0.7790],\n",
      "        [1.2141, 1.2141, 0.7850]], grad_fn=<ReluBackward0>)\n",
      "iter 25\n",
      "loss tensor(2222.5955, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7696, 0.7696, 0.7696],\n",
      "        [0.7696, 0.7696, 0.7696],\n",
      "        [1.2242, 1.2242, 0.7748]], grad_fn=<ReluBackward0>)\n",
      "iter 26\n",
      "loss tensor(2228.5596, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7599, 0.7599, 0.7599],\n",
      "        [0.7599, 0.7599, 0.7599],\n",
      "        [1.2343, 1.2343, 0.7645]], grad_fn=<ReluBackward0>)\n",
      "iter 27\n",
      "loss tensor(2369.5513, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7505, 0.7505, 0.7505],\n",
      "        [0.7505, 0.7505, 0.7505],\n",
      "        [1.2445, 1.2445, 0.7543]], grad_fn=<ReluBackward0>)\n",
      "iter 28\n",
      "loss tensor(2375.3113, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7408, 0.7408, 0.7408],\n",
      "        [0.7408, 0.7408, 0.7408],\n",
      "        [1.2546, 1.2546, 0.7441]], grad_fn=<ReluBackward0>)\n",
      "iter 29\n",
      "loss tensor(2512.6621, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7314, 0.7314, 0.7314],\n",
      "        [0.7314, 0.7314, 0.7314],\n",
      "        [1.2647, 1.2647, 0.7338]], grad_fn=<ReluBackward0>)\n",
      "iter 30\n",
      "loss tensor(2518.2183, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.7217, 0.7217, 0.7217],\n",
      "        [0.7217, 0.7217, 0.7217],\n",
      "        [1.2748, 1.2748, 0.7236]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# need products 3 and 4 to make 5\n",
    "# att weights should have nonzero in 3,1 and/or 3,2 - this works\n",
    "t.autograd.set_detect_anomaly(True)\n",
    "for i in range(1, 31):\n",
    "    opt.zero_grad()\n",
    "    if (i % 2) == 0:\n",
    "        # 1 sells 5 to 2\n",
    "        src = t.Tensor([1]).long()\n",
    "        dst = t.Tensor([2]).long()\n",
    "        prod = t.Tensor([5]).long()\n",
    "        time = t.Tensor([i]).long()\n",
    "        raw_msg = t.Tensor([1]).reshape(-1, 1)\n",
    "    else:\n",
    "        # 1 buys 3 and 4 from 0\n",
    "        src = t.Tensor([0, 0]).long()\n",
    "        dst = t.Tensor([1, 1]).long()\n",
    "        prod = t.Tensor([3, 4]).long()\n",
    "        time = t.Tensor([i, i]).long()\n",
    "        raw_msg = t.Tensor([2, 4]).reshape(-1, 1)\n",
    "\n",
    "    print('iter', i)\n",
    "    mem.update_state(src, dst, prod, time, raw_msg)\n",
    "    memory, last_update, loss = mem(n_id)\n",
    "    print('loss', loss)\n",
    "    att_weights = mem.get_prod_attention()\n",
    "    print('att weights', att_weights)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    mem.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 2\n",
      "prod_bilinear torch.Size([2, 2])\n",
      "msg_s_module.lin.weight torch.Size([17, 17])\n",
      "msg_s_module.lin.bias torch.Size([17])\n",
      "msg_s_module.layer_norm.weight torch.Size([17])\n",
      "msg_s_module.layer_norm.bias torch.Size([17])\n",
      "msg_d_module.lin.weight torch.Size([17, 17])\n",
      "msg_d_module.lin.bias torch.Size([17])\n",
      "msg_d_module.layer_norm.weight torch.Size([17])\n",
      "msg_d_module.layer_norm.bias torch.Size([17])\n",
      "msg_p_module.lin.weight torch.Size([17, 17])\n",
      "msg_p_module.lin.bias torch.Size([17])\n",
      "msg_p_module.layer_norm.weight torch.Size([17])\n",
      "msg_p_module.layer_norm.bias torch.Size([17])\n",
      "time_enc.lin.weight torch.Size([1, 1])\n",
      "time_enc.lin.bias torch.Size([1])\n",
      "state_updater.weight_ih torch.Size([6, 17])\n",
      "state_updater.weight_hh torch.Size([6, 2])\n",
      "state_updater.bias_ih torch.Size([6])\n",
      "state_updater.bias_hh torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# test learning attention weights with product memories\n",
    "mem = TGNPLMemory(num_nodes,\n",
    "        num_prods,\n",
    "        raw_msg_dim,\n",
    "        state_dim,\n",
    "        time_dim,\n",
    "        message_module,\n",
    "        aggregator_module,\n",
    "        state_updater_cell=\"gru\",\n",
    "        use_inventory=True,\n",
    "        learn_att_direct=False,\n",
    "        debt_penalty=10,\n",
    "        consumption_reward=1,\n",
    "        debug=False)\n",
    "opt = t.optim.Adam(mem.parameters(), lr=1e-2)\n",
    "for name, param in mem.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "loss tensor(0.4768, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0044, 0.0044, 0.0000],\n",
      "        [0.0044, 0.0044, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 2\n",
      "loss tensor(0.4148, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0031, 0.0031, 0.0015],\n",
      "        [0.0031, 0.0031, 0.0015],\n",
      "        [0.0015, 0.0015, 0.0005]], grad_fn=<ReluBackward0>)\n",
      "iter 3\n",
      "loss tensor(3.2606, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.1207, 0.0161, 0.0000],\n",
      "        [0.0161, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 4\n",
      "loss tensor(3.2100, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.1169, 0.0131, 0.0000],\n",
      "        [0.0131, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0202]], grad_fn=<ReluBackward0>)\n",
      "iter 5\n",
      "loss tensor(15.2349, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.1803, 0.1137, 0.0000],\n",
      "        [0.1137, 0.0697, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0142]], grad_fn=<ReluBackward0>)\n",
      "iter 6\n",
      "loss tensor(14.6644, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.1751, 0.1087, 0.0000],\n",
      "        [0.1088, 0.0651, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 7\n",
      "loss tensor(22.0765, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0960, 0.0653, 0.0000],\n",
      "        [0.0653, 0.0419, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 8\n",
      "loss tensor(21.3169, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0887, 0.0582, 0.0000],\n",
      "        [0.0582, 0.0351, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 9\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 10\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 11\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 12\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 13\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 14\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 15\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 16\n",
      "loss tensor(21.1218, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 17\n",
      "loss tensor(23.8751, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0174, 0.0243, 0.0000],\n",
      "        [0.0243, 0.0313, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 18\n",
      "loss tensor(22.9288, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0085, 0.0156, 0.0000],\n",
      "        [0.0156, 0.0226, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "iter 19\n",
      "loss tensor(21.9199, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 20\n",
      "loss tensor(21.9199, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 21\n",
      "loss tensor(21.9199, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 22\n",
      "loss tensor(21.9918, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0080]], grad_fn=<ReluBackward0>)\n",
      "iter 23\n",
      "loss tensor(22.0076, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0046]], grad_fn=<ReluBackward0>)\n",
      "iter 24\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 25\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 26\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 27\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 28\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 29\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "iter 30\n",
      "loss tensor(21.9800, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# need products 3 and 4 to make 5\n",
    "# att weights should have nonzero in 3,1 and/or 3,2 - this doesn't work\n",
    "t.autograd.set_detect_anomaly(True)\n",
    "for i in range(1, 31):\n",
    "    opt.zero_grad()\n",
    "    if (i % 2) == 0:\n",
    "        # 1 sells 5 to 2\n",
    "        src = t.Tensor([1]).long()\n",
    "        dst = t.Tensor([2]).long()\n",
    "        prod = t.Tensor([5]).long()\n",
    "        time = t.Tensor([i]).long()\n",
    "        raw_msg = t.Tensor([1]).reshape(-1, 1)\n",
    "    else:\n",
    "        # 1 buys 3 and 4 from 0\n",
    "        src = t.Tensor([0, 0]).long()\n",
    "        dst = t.Tensor([1, 1]).long()\n",
    "        prod = t.Tensor([3, 4]).long()\n",
    "        time = t.Tensor([i, i]).long()\n",
    "        raw_msg = t.Tensor([2, 4]).reshape(-1, 1)\n",
    "\n",
    "    print('iter', i)\n",
    "    mem.update_state(src, dst, prod, time, raw_msg)\n",
    "    memory, last_update, loss = mem(n_id)\n",
    "    print('loss', loss)\n",
    "    att_weights = mem.get_prod_attention()\n",
    "    print('att weights', att_weights)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    mem.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Neighbor Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "torch.Size([9, 2])\n",
      "torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "# Test init\n",
    "num_nodes = 9\n",
    "num_neighbors = 2\n",
    "all_nid = torch.arange(9).long()\n",
    "fid = torch.arange(6).long()  # firm ids\n",
    "pid = torch.arange(6,9).long()  # prod ids\n",
    "neighbor_loader = LastNeighborLoaderTGNPL(num_nodes, size=num_neighbors)\n",
    "print(neighbor_loader.neighbors.shape)\n",
    "print(neighbor_loader.e_id.shape)\n",
    "print(neighbor_loader._assoc.shape)\n",
    "self = neighbor_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor([6, 0]) tensor([ 0, -1])\n",
      "tensor(1) tensor([7, 1]) tensor([ 2, -1])\n",
      "tensor(2) tensor([8, 0]) tensor([ 4, -1])\n",
      "tensor(3) tensor([              6, 140133897100152]) tensor([ 1, -1])\n",
      "tensor(4) tensor([  7, 129]) tensor([ 3, -1])\n",
      "tensor(5) tensor([              8, 140133897100152]) tensor([ 5, -1])\n",
      "tensor(6) tensor([3, 0]) tensor([1, 0])\n",
      "tensor(7) tensor([4, 1]) tensor([3, 2])\n",
      "tensor(8) tensor([5, 2]) tensor([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test insert\n",
    "src = torch.Tensor([0, 1, 2]).to(torch.long)\n",
    "dst = torch.Tensor([3, 4, 5]).to(torch.long)\n",
    "prod = torch.Tensor([6, 7, 8]).to(torch.long)\n",
    "neighbor_loader.insert(src, dst, prod)\n",
    "\n",
    "# Ground truth neighbors\n",
    "# 0: 6; 1: 7; 2: 8; 3: 6; 4: 7; 5: 8; 6: 3,0; 7: 4,1; 8: 5,2\n",
    "for i in all_nid:\n",
    "    print(i, self.neighbors[i], self.e_id[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8]) torch.Size([9])\n",
      "torch.Size([2, 12]) torch.Size([12])\n",
      "6 -> 0; e_id = 0\n",
      "7 -> 1; e_id = 2\n",
      "8 -> 2; e_id = 4\n",
      "6 -> 3; e_id = 1\n",
      "7 -> 4; e_id = 3\n",
      "8 -> 5; e_id = 5\n",
      "3 -> 6; e_id = 1\n",
      "0 -> 6; e_id = 0\n",
      "4 -> 7; e_id = 3\n",
      "1 -> 7; e_id = 2\n",
      "5 -> 8; e_id = 5\n",
      "2 -> 8; e_id = 4\n"
     ]
    }
   ],
   "source": [
    "# Test _call_\n",
    "n_id, edge_index, e_id = neighbor_loader(fid, pid)\n",
    "# Ground truth edges (x2, for all)\n",
    "# 0-6, 3-6, 1-7, 4-7, 2-8, 5-8\n",
    "print(n_id, n_id.shape)\n",
    "print(edge_index.shape, e_id.shape)\n",
    "for i in range(edge_index.size(1)):\n",
    "    print(f'{edge_index[0,i]} -> {edge_index[1,i]}; e_id = {e_id[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor([6, 6]) tensor([6, 0])\n",
      "tensor(1) tensor([6, 7]) tensor([7, 2])\n",
      "tensor(2) tensor([8, 0]) tensor([ 4, -1])\n",
      "tensor(3) tensor([              6, 140133897100152]) tensor([ 1, -1])\n",
      "tensor(4) tensor([  7, 129]) tensor([ 3, -1])\n",
      "tensor(5) tensor([              8, 140133897100152]) tensor([ 5, -1])\n",
      "tensor(6) tensor([1, 0]) tensor([7, 6])\n",
      "tensor(7) tensor([4, 1]) tensor([3, 2])\n",
      "tensor(8) tensor([5, 2]) tensor([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test insert again\n",
    "src = torch.Tensor([0]).to(torch.long)\n",
    "dst = torch.Tensor([1]).to(torch.long)\n",
    "prod = torch.Tensor([6]).to(torch.long)\n",
    "neighbor_loader.insert(src, dst, prod)\n",
    "\n",
    "# Ground truth neighbors - 0 and 1 have extra neighbor, 6 replaced old neighbors with new neighbors\n",
    "# 0: 6,6; 1: 6,7; 2: 8; 3: 6; 4: 7; 5: 8; 6: 1,0; 7: 4,1; 8: 5,2\n",
    "for i in all_nid:\n",
    "    print(i, self.neighbors[i], self.e_id[i])  # This is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8]) torch.Size([9])\n",
      "torch.Size([2, 14]) torch.Size([14])\n",
      "6 -> 0; e_id = 6\n",
      "6 -> 0; e_id = 0\n",
      "6 -> 1; e_id = 7\n",
      "7 -> 1; e_id = 2\n",
      "8 -> 2; e_id = 4\n",
      "6 -> 3; e_id = 1\n",
      "7 -> 4; e_id = 3\n",
      "8 -> 5; e_id = 5\n",
      "1 -> 6; e_id = 7\n",
      "0 -> 6; e_id = 6\n",
      "4 -> 7; e_id = 3\n",
      "1 -> 7; e_id = 2\n",
      "5 -> 8; e_id = 5\n",
      "2 -> 8; e_id = 4\n"
     ]
    }
   ],
   "source": [
    "# Test _call_\n",
    "n_id, edge_index, e_id = neighbor_loader(fid, pid)\n",
    "# Ground truth edges (x2, for all except first 0-6 and 3-6)\n",
    "# 0-6, 3-6, 1-7, 4-7, 2-8, 5-8, 0-6, 1-6\n",
    "print(n_id, n_id.shape)\n",
    "print(edge_index.shape, e_id.shape)\n",
    "for i in range(edge_index.size(1)):\n",
    "    print(f'{edge_index[0,i]} -> {edge_index[1,i]}; e_id = {e_id[i]}')  # This is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 6])\n",
      "1 -> 2; e_id = 7\n",
      "0 -> 2; e_id = 6\n"
     ]
    }
   ],
   "source": [
    "# Test calling a subset of nodes\n",
    "n_id, edge_index, e_id = neighbor_loader(torch.Tensor([]).long(), torch.Tensor([6]).long())\n",
    "print(n_id)\n",
    "for i in range(edge_index.size(1)):\n",
    "    print(f'{edge_index[0,i]} -> {edge_index[1,i]}; e_id = {e_id[i]}')  # This is correct - reindexing matches up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import math\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.datasets import JODIEDataset\n",
    "from torch_geometric.loader import TemporalDataLoader\n",
    "\n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "# internal imports\n",
    "from tgb.utils.utils import *\n",
    "from tgb.linkproppred.evaluate import Evaluator\n",
    "from modules.decoder import LinkPredictorTGNPL\n",
    "from modules.emb_module import GraphAttentionEmbedding\n",
    "from modules.msg_func import TGNPLMessage\n",
    "from modules.msg_agg import MeanAggregator\n",
    "from modules.neighbor_loader import LastNeighborLoaderTGNPL\n",
    "from modules.memory_module import TGNPLMemory, StaticMemory\n",
    "from modules.early_stopping import  EarlyStopMonitor\n",
    "from modules.hyper_edgebank import HyperEdgeBankPredictor, test_edgebank\n",
    "from tgb.linkproppred.dataset_pyg import PyGLinkPropPredDataset, PyGLinkPropPredDatasetHyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['product_threshold', 'id2entity', 'train_max_ts', 'val_max_ts', 'test_max_ts'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = 'tgbl-hypergraph'\n",
    "with open(f\"/lfs/turing1/0/{os.getlogin()}/supply-chains/TGB/tgb/datasets/{DATA.replace('-', '_')}/{DATA}_meta.json\",\"r\") as file:\n",
    "    METADATA = json.load(file)\n",
    "    NUM_NODES = len(METADATA[\"id2entity\"])\n",
    "METADATA.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METADATA['train_max_ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METADATA['test_max_ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7257, 2852)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_FIRMS = METADATA[\"product_threshold\"]\n",
    "NUM_PRODUCTS = NUM_NODES - NUM_FIRMS\n",
    "NUM_FIRMS, NUM_PRODUCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tgbl-hypergraph url not found, download not supported yet.\n",
      "file found, skipping download\n",
      "Dataset directory is  /lfs/turing1/0/serinac/supply-chains/TGB/tgb/datasets/tgbl_hypergraph\n",
      "loading processed file\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "device = \"cpu\"\n",
    "dataset = PyGLinkPropPredDatasetHyper(name=DATA, root=\"datasets\")\n",
    "train_mask = dataset.train_mask\n",
    "val_mask = dataset.val_mask\n",
    "test_mask = dataset.test_mask\n",
    "data = dataset.get_TemporalData()\n",
    "data = data.to(device)\n",
    "\n",
    "# for evaluation\n",
    "neg_sampler = dataset.negative_sampler\n",
    "dataset.load_val_ns()  # load validation negative samples\n",
    "metric = dataset.eval_metric\n",
    "evaluator = Evaluator(name=DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalData(src=[305277], dst=[305277], t=[305277], msg=[305277, 1], prod=[305277], y=[305277])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2246, 6686, 3109, 3791, 2124, 4738, 5023, 6768, 1081,  708])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.src[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1727, 4512, 5068, 6941, 2609, 4240, 1410, 4512, 3544, 5851])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dst[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8448, 8055, 9212, 7338, 8490, 9519, 9689, 8055, 7342, 8440])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.prod[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.t[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,  ..., 272, 272, 272])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.t[train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([318, 318, 318,  ..., 364, 364, 364])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.t[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "train_data = data[train_mask]\n",
    "val_data = data[val_mask]\n",
    "test_data = data[test_mask]\n",
    "\n",
    "train_loader = TemporalDataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "val_loader = TemporalDataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "test_loader = TemporalDataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7256\n",
      "7257 10108\n",
      "3 7253\n"
     ]
    }
   ],
   "source": [
    "# Ensure to only sample actual source, product, or destination nodes as negatives.\n",
    "min_src_idx, max_src_idx = int(data.src.min()), int(data.src.max())\n",
    "min_prod_idx, max_prod_idx = int(data.prod.min()), int(data.prod.max())\n",
    "min_dst_idx, max_dst_idx = int(data.dst.min()), int(data.dst.max())\n",
    "print(min_src_idx, max_src_idx)\n",
    "print(min_prod_idx, max_prod_idx)\n",
    "print(min_dst_idx, max_dst_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test edgebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgebank = HyperEdgeBankPredictor(NUM_FIRMS, NUM_PRODUCTS, consecutive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 46490307739, 138392770326,  64361316967,  78481986337,  43967793637,\n",
      "         98074310174, 103964873924, 140089921374,  22383525657,  14670138747])\n"
     ]
    }
   ],
   "source": [
    "idx = edgebank.convert_triplet_to_index(train_data.src, train_data.dst, train_data.prod)\n",
    "print(idx[:10])\n",
    "src, dst, prod = edgebank.convert_index_to_triplet(idx)\n",
    "assert (train_data.src == src).all()\n",
    "assert (train_data.dst == dst).all()\n",
    "assert (train_data.prod == prod).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit on 214674 edges; found 38756 unique\n"
     ]
    }
   ],
   "source": [
    "edgebank.fit(train_data.src, train_data.dst, train_data.prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 11.,  61.,   1., 213.,   0., 172.,   3.,   1., 131.,   7.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgebank.predict(val_data.src[:10], val_data.dst[:10], val_data.prod[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(38147), 44991)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = edgebank.predict(val_data.src, val_data.dst, val_data.prod, use_counts=True)\n",
    "torch.sum(y_pred > 0), len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 225/225 [00:02<00:00, 92.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3399311304092407"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_edgebank(val_loader, neg_sampler, \"val\", evaluator, metric, edgebank, use_counts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 225/225 [00:04<00:00, 51.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6819248199462891"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_edgebank(val_loader, neg_sampler, \"val\", evaluator, metric, edgebank, use_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
