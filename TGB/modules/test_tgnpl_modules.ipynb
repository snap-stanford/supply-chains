{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "from memory_module import TGNPLMemory\n",
    "from msg_func import TGNPLMessage\n",
    "from msg_agg import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test TGNPLMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 10\n",
    "num_prods = 2\n",
    "raw_msg_dim = 1\n",
    "state_dim = 10\n",
    "time_dim = 1\n",
    "message_module = TGNPLMessage(raw_msg_dim, state_dim+num_prods, time_dim)\n",
    "aggregator_module = MeanAggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test initialization\n",
    "mem = TGNPLMemory(num_nodes,\n",
    "        num_prods,\n",
    "        raw_msg_dim,\n",
    "        state_dim,\n",
    "        time_dim,\n",
    "        message_module,\n",
    "        aggregator_module,\n",
    "        state_updater_cell=\"gru\",\n",
    "        use_inventory=True,\n",
    "        debt_penalty=10,\n",
    "        consumption_reward=5,\n",
    "        debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 8, 4, 9, 1])\n",
      "Total consumed: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<IndexBackward0>)\n",
      "Total loss: tensor(0., grad_fn=<SumBackward0>)\n",
      "Total bought: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "msg_s tensor([], size=(0, 38), grad_fn=<CatBackward0>)\n",
      "msg_d tensor([], size=(0, 38), grad_fn=<CatBackward0>)\n",
      "msg_p tensor([], size=(0, 38), grad_fn=<CatBackward0>)\n",
      "aggr tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "memory tensor([[-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# before any interactions have been added\n",
    "np.random.seed(0)\n",
    "n_id = np.random.choice(num_nodes, size=5, replace=False)\n",
    "n_id = t.from_numpy(n_id)\n",
    "print(n_id)\n",
    "memory, last_update, loss = mem(n_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total consumed: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<IndexBackward0>)\n",
      "Total loss: tensor(0., grad_fn=<SumBackward0>)\n",
      "Total bought: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "msg_s tensor([], size=(0, 38), grad_fn=<CatBackward0>)\n",
      "msg_d tensor([], size=(0, 38), grad_fn=<CatBackward0>)\n",
      "msg_p tensor([], size=(0, 38), grad_fn=<CatBackward0>)\n",
      "aggr tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "memory tensor([[-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# try adding interactions\n",
    "# expectation: \n",
    "# 1) get_updated_memory will print 6 nodes, but memories shouldn't be updated yet\n",
    "# 2) _update_msg_store should update all three msg stores\n",
    "src = t.Tensor([0, 0, 1, 2]).long()\n",
    "dst = t.Tensor([3, 3, 3, 0]).long()\n",
    "prod = t.Tensor([8, 8, 8, 9]).long()\n",
    "time = t.Tensor(np.ones(4)).long()\n",
    "raw_msg = t.Tensor([10, 20, 5, 13]).reshape(-1, 1)\n",
    "mem.update_state(src, dst, prod, time, raw_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (tensor([0, 0]),\n",
       "  tensor([3, 3]),\n",
       "  tensor([8, 8]),\n",
       "  tensor([1, 1]),\n",
       "  tensor([[10.],\n",
       "          [20.]])),\n",
       " 1: (tensor([1]), tensor([3]), tensor([8]), tensor([1]), tensor([[5.]])),\n",
       " 2: (tensor([2]), tensor([0]), tensor([9]), tensor([1]), tensor([[13.]])),\n",
       " 3: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 4: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 5: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 6: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 7: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 8: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 9: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1)))}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.msg_s_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (tensor([2]), tensor([0]), tensor([9]), tensor([1]), tensor([[13.]])),\n",
       " 1: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 2: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 3: (tensor([0, 0, 1]),\n",
       "  tensor([3, 3, 3]),\n",
       "  tensor([8, 8, 8]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([[10.],\n",
       "          [20.],\n",
       "          [ 5.]])),\n",
       " 4: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 5: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 6: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 7: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 8: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 9: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1)))}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.msg_d_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 1: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 2: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 3: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 4: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 5: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 6: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 7: (tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], dtype=torch.int64),\n",
       "  tensor([], size=(0, 1))),\n",
       " 8: (tensor([0, 0, 1]),\n",
       "  tensor([3, 3, 3]),\n",
       "  tensor([8, 8, 8]),\n",
       "  tensor([1, 1, 1]),\n",
       "  tensor([[10.],\n",
       "          [20.],\n",
       "          [ 5.]])),\n",
       " 9: (tensor([2]), tensor([0]), tensor([9]), tensor([1]), tensor([[13.]]))}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem.msg_p_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total consumed: tensor([[3.4784, 3.4784],\n",
      "        [0.5797, 0.5797],\n",
      "        [1.5073, 1.5073],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<IndexBackward0>)\n",
      "Total loss: tensor(1057.4208, grad_fn=<SumBackward0>)\n",
      "Total bought: tensor([[ 0., 13.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [35.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.]])\n",
      "msg_s tensor([[-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 10.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 20.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000,  5.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 13.0000,  0.2286]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "msg_d tensor([[-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 13.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 10.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 20.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000,  5.0000,  0.2286]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "msg_p tensor([[-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 10.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 20.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000,  5.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 13.0000,  0.2286]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "aggr tensor([[-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 14.3333,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000,  5.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 13.0000,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 11.6667,  0.2286],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 11.6667,  0.2286],\n",
      "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, -0.2203,  0.0208,  0.1465, -0.1105,\n",
      "          0.1140,  0.0959,  0.1108, -0.0654,  0.0546,  0.0865,  0.0000,  0.0000,\n",
      "         -0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
      "          0.0546,  0.0865,  0.0000,  0.0000, 13.0000,  0.2286]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "memory tensor([[ 4.3592e-01, -5.5804e-01,  6.6407e-02, -1.0760e-01,  8.6523e-01,\n",
      "          6.9769e-01,  7.3195e-03, -6.7096e-01,  3.5679e-02,  9.9171e-01,\n",
      "         -3.4784e+00,  9.5216e+00],\n",
      "        [-4.7474e-02, -4.3409e-01, -5.8806e-02, -1.1718e-01,  5.6522e-01,\n",
      "          4.5913e-01, -1.7055e-01, -3.8583e-01, -4.0209e-02,  7.9342e-01,\n",
      "         -5.7973e-01, -5.7973e-01],\n",
      "        [ 4.0338e-01, -5.5169e-01,  4.7263e-02, -1.0733e-01,  8.4124e-01,\n",
      "          6.8083e-01, -1.3952e-02, -6.4154e-01,  2.8400e-02,  9.8694e-01,\n",
      "         -1.5073e+00, -1.5073e+00],\n",
      "        [ 3.6127e-01, -5.4452e-01,  2.5470e-02, -1.0723e-01,  8.1289e-01,\n",
      "          6.6089e-01, -3.8717e-02, -6.0877e-01,  1.8932e-02,  9.7933e-01,\n",
      "          3.5000e+01,  0.0000e+00],\n",
      "        [-2.2030e-01,  2.0767e-02,  1.4648e-01, -1.1049e-01,  1.1400e-01,\n",
      "          9.5871e-02,  1.1082e-01, -6.5379e-02,  5.4565e-02,  8.6472e-02,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.2030e-01,  2.0767e-02,  1.4648e-01, -1.1049e-01,  1.1400e-01,\n",
      "          9.5871e-02,  1.1082e-01, -6.5379e-02,  5.4565e-02,  8.6472e-02,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.2030e-01,  2.0767e-02,  1.4648e-01, -1.1049e-01,  1.1400e-01,\n",
      "          9.5871e-02,  1.1082e-01, -6.5379e-02,  5.4565e-02,  8.6472e-02,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.2030e-01,  2.0767e-02,  1.4648e-01, -1.1049e-01,  1.1400e-01,\n",
      "          9.5871e-02,  1.1082e-01, -6.5379e-02,  5.4565e-02,  8.6472e-02,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 3.6127e-01, -5.4452e-01,  2.5470e-02, -1.0723e-01,  8.1289e-01,\n",
      "          6.6089e-01, -3.8717e-02, -6.0877e-01,  1.8932e-02,  9.7933e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 4.0338e-01, -5.5169e-01,  4.7263e-02, -1.0733e-01,  8.4124e-01,\n",
      "          6.8083e-01, -1.3952e-02, -6.4154e-01,  2.8400e-02,  9.8694e-01,\n",
      "          0.0000e+00,  0.0000e+00]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# now get memory again - only nodes with interactions should've changed\n",
    "n_id = t.from_numpy(np.arange(num_nodes))\n",
    "memory, last_update, loss = mem(n_id)  # test .forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4034, -0.5517,  0.0473, -0.1073,  0.8412,  0.6808, -0.0140, -0.6415,\n",
       "          0.0284,  0.9869, -1.5073, -1.5073],\n",
       "        [ 0.4034, -0.5517,  0.0473, -0.1073,  0.8412,  0.6808, -0.0140, -0.6415,\n",
       "          0.0284,  0.9869,  0.0000,  0.0000]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 and 9, same state, different inventory\n",
    "# 2 supplied product 9\n",
    "# product 9 has no inventory\n",
    "memory[[2,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6127e-01, -5.4452e-01,  2.5470e-02, -1.0723e-01,  8.1289e-01,\n",
       "          6.6089e-01, -3.8717e-02, -6.0877e-01,  1.8932e-02,  9.7933e-01,\n",
       "          3.5000e+01,  0.0000e+00],\n",
       "        [ 3.6127e-01, -5.4452e-01,  2.5470e-02, -1.0723e-01,  8.1289e-01,\n",
       "          6.6089e-01, -3.8717e-02, -6.0877e-01,  1.8932e-02,  9.7933e-01,\n",
       "          0.0000e+00,  0.0000e+00]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 and 8, same state, different inventory\n",
    "# 3 received exactly 35 of product 8\n",
    "# product 8 has no inventory\n",
    "memory[[3,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
       "          0.0546,  0.0865,  0.0000,  0.0000],\n",
       "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
       "          0.0546,  0.0865,  0.0000,  0.0000],\n",
       "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
       "          0.0546,  0.0865,  0.0000,  0.0000],\n",
       "        [-0.2203,  0.0208,  0.1465, -0.1105,  0.1140,  0.0959,  0.1108, -0.0654,\n",
       "          0.0546,  0.0865,  0.0000,  0.0000]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be unaffected\n",
    "memory[[4,5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should only be updated for nodes in transactions\n",
    "last_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test attention weight learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 6\n",
    "num_prods = 3\n",
    "raw_msg_dim = 1\n",
    "state_dim = 2\n",
    "time_dim = 1\n",
    "message_module = TGNPLMessage(raw_msg_dim, state_dim+num_prods, time_dim)\n",
    "aggregator_module = MeanAggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_msg_dim + (3 * memory_dim) + time_dim\n",
    "message_module.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "n_id = t.arange(0, num_nodes).long()\n",
    "print(n_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_enc.lin.weight torch.Size([1, 1])\n",
      "time_enc.lin.bias torch.Size([1])\n",
      "state_updater.weight_ih torch.Size([6, 17])\n",
      "state_updater.weight_hh torch.Size([6, 2])\n",
      "state_updater.bias_ih torch.Size([6])\n",
      "state_updater.bias_hh torch.Size([6])\n",
      "output_l.weight torch.Size([2, 2])\n",
      "output_l.bias torch.Size([2])\n",
      "input_l.weight torch.Size([2, 2])\n",
      "input_l.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "mem = TGNPLMemory(num_nodes,\n",
    "        num_prods,\n",
    "        raw_msg_dim,\n",
    "        state_dim,\n",
    "        time_dim,\n",
    "        message_module,\n",
    "        aggregator_module,\n",
    "        state_updater_cell=\"gru\",\n",
    "        use_inventory=True,\n",
    "        debt_penalty=10,\n",
    "        consumption_reward=5,\n",
    "        debug=False)\n",
    "opt = t.optim.Adam(mem.parameters())\n",
    "for name, param in mem.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "loss tensor(125.1899, grad_fn=<SumBackward0>)\n",
      "att weights tensor([[0.1329, 0.1329, 0.1135],\n",
      "        [0.1329, 0.1329, 0.1135],\n",
      "        [0.0869, 0.0869, 0.0669]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 2]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_68135/622664421.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0matt_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'att weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/serinac/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m/lfs/local/0/serinac/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 2]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "t.autograd.set_detect_anomaly(True)\n",
    "for i in range(1, 31):\n",
    "#     opt.zero_grad()\n",
    "    if (i % 2) == 0:\n",
    "        # 1 sells 5 to 2\n",
    "        src = t.Tensor([1]).long()\n",
    "        dst = t.Tensor([2]).long()\n",
    "        prod = t.Tensor([5]).long()\n",
    "        time = t.Tensor([i]).long()\n",
    "        raw_msg = t.Tensor([1]).reshape(-1, 1)\n",
    "    else:\n",
    "        # 1 buys 3 and 4 from 0\n",
    "        src = t.Tensor([0, 0]).long()\n",
    "        dst = t.Tensor([1, 1]).long()\n",
    "        prod = t.Tensor([3, 4]).long()\n",
    "        time = t.Tensor([i, i]).long()\n",
    "        raw_msg = t.Tensor([2, 4]).reshape(-1, 1)\n",
    "\n",
    "    print('iter', i)\n",
    "    mem.update_state(src, dst, prod, time, raw_msg)\n",
    "    memory, last_update, loss = mem(n_id)\n",
    "    print('loss', loss)\n",
    "    prod_emb = mem.memory[mem.num_firms:, :mem.state_dim]\n",
    "    # prod_emb = t.ones(mem.num_prods, mem.state_dim)\n",
    "    output_emb = mem.output_l(prod_emb)  # num_products x emb_dim\n",
    "    input_emb = mem.input_l(prod_emb)  # num_products x emb_dim\n",
    "    att_weights = output_emb @ input_emb.T  # num_products x num_products\n",
    "    att_weights = t.nn.ReLU(inplace=False)(att_weights)\n",
    "    print('att weights', att_weights)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supply_chain_venv",
   "language": "python",
   "name": "supply_chain_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
